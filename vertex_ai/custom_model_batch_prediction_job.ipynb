{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Y5yTxMNOKnOC"
   },
   "outputs": [],
   "source": [
    "# Copyright 2024 Google LLC\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#     https://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ndfGn8c4bI_G"
   },
   "source": [
    "# Model Monitoring for Vertex AI Custom Model Batch Prediction Job\n",
    "\n",
    "<table align=\"left\">\n",
    "  <td style=\"text-align: center\">\n",
    "    <a href=\"https://colab.research.google.com/github/GoogleCloudPlatform/vertex-ai-samples/blob/main/notebooks/official/model_monitoring_v2/model_monitoring_for_custom_model_batch_prediction_job.ipynb\">\n",
    "      <img src=\"https://cloud.google.com/ml-engine/images/colab-logo-32px.png\" alt=\"Google Colaboratory logo\"><br> Open in Colab\n",
    "    </a>\n",
    "  </td>\n",
    "  <td style=\"text-align: center\">\n",
    "    <a href=\"https://console.cloud.google.com/vertex-ai/colab/import/https:%2F%2Fraw.githubusercontent.com%2FGoogleCloudPlatform%2Fvertex-ai-samples%2Fmain%2Fnotebooks%2Fofficial%2model_monitoring_v2%2model_monitoring_for_custom_model_batch_prediction_job.ipynb\">\n",
    "      <img width=\"32px\" src=\"https://lh3.googleusercontent.com/JmcxdQi-qOpctIvWKgPtrzZdJJK-J3sWE1RsfjZNwshCFgE_9fULcNpuXYTilIR2hjwN\" alt=\"Google Cloud Colab Enterprise logo\"><br> Open in Colab Enterprise\n",
    "    </a>\n",
    "  </td>    \n",
    "  <td style=\"text-align: center\">\n",
    "    <a href=\"https://console.cloud.google.com/vertex-ai/workbench/deploy-notebook?download_url=https://raw.githubusercontent.com/GoogleCloudPlatform/vertex-ai-samples/blob/main/notebooks/official/model_monitoring_v2/model_monitoring_for_custom_model_batch_prediction_job.ipynb\">\n",
    "      <img src=\"https://lh3.googleusercontent.com/UiNooY4LUgW_oTvpsNhPpQzsstV5W8F7rYgxgGBD85cWJoLmrOzhVs_ksK_vgx40SHs7jCqkTkCk=e14-rj-sc0xffffff-h130-w32\" alt=\"Vertex AI logo\"><br> Open in Workbench\n",
    "    </a>\n",
    "  </td>\n",
    "  <td style=\"text-align: center\">\n",
    "    <a href=\"https://github.com/GoogleCloudPlatform/vertex-ai-samples/blob/main/notebooks/official/model_monitoring_v2/model_monitoring_for_custom_model_batch_prediction_job.ipynb\">\n",
    "      <img src=\"https://cloud.google.com/ml-engine/images/github-logo-32px.png\" alt=\"GitHub logo\"><br> View on GitHub\n",
    "    </a>\n",
    "  </td>\n",
    "</table>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "R5Hp34AVK7ba"
   },
   "source": [
    "## Overview\n",
    "\n",
    "This tutorial demonstrates how to use the Vertex AI SDK for Python to set up Vertex AI Model Monitoring V2 for your model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DyXl559OtyK_"
   },
   "source": [
    "### Objective\n",
    "\n",
    "In this tutorial, you'll complete the following steps:\n",
    "\n",
    "- Upload a custom model to Vertex AI Model Registry.\n",
    "- Create a model monitor.\n",
    "- Create Vertex AI batch prediction job.\n",
    "- Run an on-demand model monitoring job to analyze data drift between the batch prediction job results and the training dataset.\n",
    "- Create another Vertex AI batch prediction job.\n",
    "- Run an on-demand model monitoring job to analyze data drift between the batch prediction job results and the previous batch prediction job.\n",
    "- Run an on-demand model monitoring job to analyze the feature attribution drift between the batch prediction job results and a baseline dataset in Google Cloud Storage.\n",
    "\n",
    "\n",
    "### Costs\n",
    "\n",
    "Vertex AI Model Monitoring v2 is free during the public preview period, but you will still be billed for the following Google Cloud services:\n",
    "\n",
    "* [BigQuery](https://cloud.google.com/bigquery/pricing)\n",
    "* [Cloud Storage](https://cloud.google.com/storage/pricing)\n",
    "* [Vertex AI Online Prediction](https://cloud.google.com/vertex-ai/pricing#prediction-prices)\n",
    "* [Vertex AI Batch Explanation Job](https://cloud.google.com/vertex-ai/pricing#prediction-prices) (if you run the feature attribution drift example)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oYmaoz9MNLHd"
   },
   "source": [
    "## Getting Started"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NvW5zNXmNMwm"
   },
   "source": [
    "### Install Vertex AI SDK and other required packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "5FakhYktNO7Y",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: google-cloud-aiplatform in /opt/conda/lib/python3.10/site-packages (1.57.0)\n",
      "Collecting google-cloud-aiplatform\n",
      "  Downloading google_cloud_aiplatform-1.58.0-py2.py3-none-any.whl.metadata (31 kB)\n",
      "Requirement already satisfied: google-cloud-bigquery in /opt/conda/lib/python3.10/site-packages (3.25.0)\n",
      "Requirement already satisfied: kfp in /home/jupyter/.local/lib/python3.10/site-packages (2.7.0)\n",
      "Collecting kfp\n",
      "  Using cached kfp-2.8.0.tar.gz (594 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: pandas in /opt/conda/lib/python3.10/site-packages (1.5.3)\n",
      "Collecting pandas\n",
      "  Using cached pandas-2.2.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (19 kB)\n",
      "Requirement already satisfied: pandas_gbq in /opt/conda/lib/python3.10/site-packages (0.23.1)\n",
      "Requirement already satisfied: pyarrow in /opt/conda/lib/python3.10/site-packages (10.0.1)\n",
      "Collecting pyarrow\n",
      "  Using cached pyarrow-16.1.0-cp310-cp310-manylinux_2_28_x86_64.whl.metadata (3.0 kB)\n",
      "Requirement already satisfied: tensorflow_data_validation[visualization] in /opt/conda/lib/python3.10/site-packages (1.15.1)\n",
      "Requirement already satisfied: google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.34.1 in /home/jupyter/.local/lib/python3.10/site-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.34.1->google-cloud-aiplatform) (2.19.0)\n",
      "Requirement already satisfied: google-auth<3.0.0dev,>=2.14.1 in /home/jupyter/.local/lib/python3.10/site-packages (from google-cloud-aiplatform) (2.29.0)\n",
      "Requirement already satisfied: proto-plus<2.0.0dev,>=1.22.0 in /opt/conda/lib/python3.10/site-packages (from google-cloud-aiplatform) (1.23.0)\n",
      "Requirement already satisfied: protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5 in /opt/conda/lib/python3.10/site-packages (from google-cloud-aiplatform) (3.20.3)\n",
      "Requirement already satisfied: packaging>=14.3 in /opt/conda/lib/python3.10/site-packages (from google-cloud-aiplatform) (24.0)\n",
      "Requirement already satisfied: google-cloud-storage<3.0.0dev,>=1.32.0 in /opt/conda/lib/python3.10/site-packages (from google-cloud-aiplatform) (2.14.0)\n",
      "Requirement already satisfied: google-cloud-resource-manager<3.0.0dev,>=1.3.3 in /opt/conda/lib/python3.10/site-packages (from google-cloud-aiplatform) (1.12.3)\n",
      "Requirement already satisfied: shapely<3.0.0dev in /opt/conda/lib/python3.10/site-packages (from google-cloud-aiplatform) (2.0.4)\n",
      "Requirement already satisfied: pydantic<3 in /opt/conda/lib/python3.10/site-packages (from google-cloud-aiplatform) (2.7.1)\n",
      "Requirement already satisfied: docstring-parser<1 in /opt/conda/lib/python3.10/site-packages (from google-cloud-aiplatform) (0.16)\n",
      "Requirement already satisfied: google-cloud-core<3.0.0dev,>=1.6.0 in /opt/conda/lib/python3.10/site-packages (from google-cloud-bigquery) (2.4.1)\n",
      "Requirement already satisfied: google-resumable-media<3.0dev,>=0.6.0 in /opt/conda/lib/python3.10/site-packages (from google-cloud-bigquery) (2.7.0)\n",
      "Requirement already satisfied: python-dateutil<3.0dev,>=2.7.2 in /opt/conda/lib/python3.10/site-packages (from google-cloud-bigquery) (2.9.0)\n",
      "Requirement already satisfied: requests<3.0.0dev,>=2.21.0 in /opt/conda/lib/python3.10/site-packages (from google-cloud-bigquery) (2.31.0)\n",
      "Requirement already satisfied: click<9,>=8.0.0 in /opt/conda/lib/python3.10/site-packages (from kfp) (8.1.7)\n",
      "Requirement already satisfied: kfp-pipeline-spec==0.3.0 in /home/jupyter/.local/lib/python3.10/site-packages (from kfp) (0.3.0)\n",
      "Requirement already satisfied: kfp-server-api<2.1.0,>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from kfp) (2.0.5)\n",
      "Requirement already satisfied: kubernetes<27,>=8.0.0 in /opt/conda/lib/python3.10/site-packages (from kfp) (26.1.0)\n",
      "Collecting protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5 (from google-cloud-aiplatform)\n",
      "  Using cached protobuf-4.25.3-cp37-abi3-manylinux2014_x86_64.whl.metadata (541 bytes)\n",
      "Requirement already satisfied: PyYAML<7,>=5.3 in /opt/conda/lib/python3.10/site-packages (from kfp) (6.0.1)\n",
      "Requirement already satisfied: requests-toolbelt<1,>=0.8.0 in /opt/conda/lib/python3.10/site-packages (from kfp) (0.10.1)\n",
      "Requirement already satisfied: tabulate<1,>=0.8.6 in /opt/conda/lib/python3.10/site-packages (from kfp) (0.9.0)\n",
      "Requirement already satisfied: urllib3<2.0.0 in /opt/conda/lib/python3.10/site-packages (from kfp) (1.26.18)\n",
      "Requirement already satisfied: numpy>=1.22.4 in /opt/conda/lib/python3.10/site-packages (from pandas) (1.24.4)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /opt/conda/lib/python3.10/site-packages (from pandas) (2024.1)\n",
      "Requirement already satisfied: setuptools in /opt/conda/lib/python3.10/site-packages (from pandas_gbq) (69.5.1)\n",
      "Requirement already satisfied: db-dtypes<2.0.0,>=1.0.4 in /opt/conda/lib/python3.10/site-packages (from pandas_gbq) (1.2.0)\n",
      "Requirement already satisfied: pydata-google-auth>=1.5.0 in /opt/conda/lib/python3.10/site-packages (from pandas_gbq) (1.8.2)\n",
      "Requirement already satisfied: google-auth-oauthlib>=0.7.0 in /opt/conda/lib/python3.10/site-packages (from pandas_gbq) (1.2.0)\n",
      "Requirement already satisfied: absl-py<2.0.0,>=0.9 in /opt/conda/lib/python3.10/site-packages (from tensorflow_data_validation[visualization]) (1.4.0)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow_data_validation[visualization]) (1.4.2)\n",
      "Requirement already satisfied: pyfarmhash<0.4,>=0.2.2 in /opt/conda/lib/python3.10/site-packages (from tensorflow_data_validation[visualization]) (0.3.2)\n",
      "Requirement already satisfied: six<2,>=1.12 in /opt/conda/lib/python3.10/site-packages (from tensorflow_data_validation[visualization]) (1.16.0)\n",
      "Requirement already satisfied: tensorflow<2.16,>=2.15 in /opt/conda/lib/python3.10/site-packages (from tensorflow_data_validation[visualization]) (2.15.1)\n",
      "Requirement already satisfied: tensorflow-metadata<1.16,>=1.15.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow_data_validation[visualization]) (1.15.0)\n",
      "Requirement already satisfied: tfx-bsl<1.16,>=1.15.1 in /opt/conda/lib/python3.10/site-packages (from tensorflow_data_validation[visualization]) (1.15.1)\n",
      "Requirement already satisfied: apache-beam<3,>=2.47 in /opt/conda/lib/python3.10/site-packages (from apache-beam[gcp]<3,>=2.47; python_version < \"3.11\"->tensorflow_data_validation[visualization]) (2.50.0)\n",
      "Requirement already satisfied: ipython<8,>=7 in /opt/conda/lib/python3.10/site-packages (from tensorflow_data_validation[visualization]) (7.34.0)\n",
      "Requirement already satisfied: crcmod<2.0,>=1.7 in /opt/conda/lib/python3.10/site-packages (from apache-beam<3,>=2.47->apache-beam[gcp]<3,>=2.47; python_version < \"3.11\"->tensorflow_data_validation[visualization]) (1.7)\n",
      "Requirement already satisfied: orjson<4.0 in /opt/conda/lib/python3.10/site-packages (from apache-beam<3,>=2.47->apache-beam[gcp]<3,>=2.47; python_version < \"3.11\"->tensorflow_data_validation[visualization]) (3.10.3)\n",
      "Requirement already satisfied: dill<0.3.2,>=0.3.1.1 in /opt/conda/lib/python3.10/site-packages (from apache-beam<3,>=2.47->apache-beam[gcp]<3,>=2.47; python_version < \"3.11\"->tensorflow_data_validation[visualization]) (0.3.1.1)\n",
      "Requirement already satisfied: cloudpickle~=2.2.1 in /opt/conda/lib/python3.10/site-packages (from apache-beam<3,>=2.47->apache-beam[gcp]<3,>=2.47; python_version < \"3.11\"->tensorflow_data_validation[visualization]) (2.2.1)\n",
      "Requirement already satisfied: fastavro<2,>=0.23.6 in /opt/conda/lib/python3.10/site-packages (from apache-beam<3,>=2.47->apache-beam[gcp]<3,>=2.47; python_version < \"3.11\"->tensorflow_data_validation[visualization]) (1.9.4)\n",
      "Requirement already satisfied: fasteners<1.0,>=0.3 in /opt/conda/lib/python3.10/site-packages (from apache-beam<3,>=2.47->apache-beam[gcp]<3,>=2.47; python_version < \"3.11\"->tensorflow_data_validation[visualization]) (0.19)\n",
      "Requirement already satisfied: grpcio!=1.48.0,<2,>=1.33.1 in /opt/conda/lib/python3.10/site-packages (from apache-beam<3,>=2.47->apache-beam[gcp]<3,>=2.47; python_version < \"3.11\"->tensorflow_data_validation[visualization]) (1.63.0)\n",
      "Requirement already satisfied: hdfs<3.0.0,>=2.1.0 in /opt/conda/lib/python3.10/site-packages (from apache-beam<3,>=2.47->apache-beam[gcp]<3,>=2.47; python_version < \"3.11\"->tensorflow_data_validation[visualization]) (2.7.3)\n",
      "Requirement already satisfied: httplib2<0.23.0,>=0.8 in /opt/conda/lib/python3.10/site-packages (from apache-beam<3,>=2.47->apache-beam[gcp]<3,>=2.47; python_version < \"3.11\"->tensorflow_data_validation[visualization]) (0.22.0)\n",
      "Requirement already satisfied: objsize<0.7.0,>=0.6.1 in /opt/conda/lib/python3.10/site-packages (from apache-beam<3,>=2.47->apache-beam[gcp]<3,>=2.47; python_version < \"3.11\"->tensorflow_data_validation[visualization]) (0.6.1)\n",
      "Requirement already satisfied: pymongo<5.0.0,>=3.8.0 in /opt/conda/lib/python3.10/site-packages (from apache-beam<3,>=2.47->apache-beam[gcp]<3,>=2.47; python_version < \"3.11\"->tensorflow_data_validation[visualization]) (4.7.2)\n",
      "  Using cached protobuf-4.23.4-cp37-abi3-manylinux2014_x86_64.whl.metadata (540 bytes)\n",
      "Requirement already satisfied: pydot<2,>=1.2.0 in /opt/conda/lib/python3.10/site-packages (from apache-beam<3,>=2.47->apache-beam[gcp]<3,>=2.47; python_version < \"3.11\"->tensorflow_data_validation[visualization]) (1.4.2)\n",
      "Requirement already satisfied: regex>=2020.6.8 in /opt/conda/lib/python3.10/site-packages (from apache-beam<3,>=2.47->apache-beam[gcp]<3,>=2.47; python_version < \"3.11\"->tensorflow_data_validation[visualization]) (2024.5.15)\n",
      "Requirement already satisfied: typing-extensions>=3.7.0 in /opt/conda/lib/python3.10/site-packages (from apache-beam<3,>=2.47->apache-beam[gcp]<3,>=2.47; python_version < \"3.11\"->tensorflow_data_validation[visualization]) (4.11.0)\n",
      "Requirement already satisfied: zstandard<1,>=0.18.0 in /opt/conda/lib/python3.10/site-packages (from apache-beam<3,>=2.47->apache-beam[gcp]<3,>=2.47; python_version < \"3.11\"->tensorflow_data_validation[visualization]) (0.22.0)\n",
      "Requirement already satisfied: cachetools<6,>=3.1.0 in /opt/conda/lib/python3.10/site-packages (from apache-beam[gcp]<3,>=2.47; python_version < \"3.11\"->tensorflow_data_validation[visualization]) (4.2.4)\n",
      "Requirement already satisfied: google-apitools<0.5.32,>=0.5.31 in /opt/conda/lib/python3.10/site-packages (from apache-beam[gcp]<3,>=2.47; python_version < \"3.11\"->tensorflow_data_validation[visualization]) (0.5.31)\n",
      "Requirement already satisfied: google-auth-httplib2<0.2.0,>=0.1.0 in /opt/conda/lib/python3.10/site-packages (from apache-beam[gcp]<3,>=2.47; python_version < \"3.11\"->tensorflow_data_validation[visualization]) (0.1.1)\n",
      "Requirement already satisfied: google-cloud-datastore<3,>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from apache-beam[gcp]<3,>=2.47; python_version < \"3.11\"->tensorflow_data_validation[visualization]) (2.19.0)\n",
      "Requirement already satisfied: google-cloud-pubsub<3,>=2.1.0 in /opt/conda/lib/python3.10/site-packages (from apache-beam[gcp]<3,>=2.47; python_version < \"3.11\"->tensorflow_data_validation[visualization]) (2.21.1)\n",
      "Requirement already satisfied: google-cloud-pubsublite<2,>=1.2.0 in /opt/conda/lib/python3.10/site-packages (from apache-beam[gcp]<3,>=2.47; python_version < \"3.11\"->tensorflow_data_validation[visualization]) (1.10.0)\n",
      "Requirement already satisfied: google-cloud-bigquery-storage<3,>=2.6.3 in /opt/conda/lib/python3.10/site-packages (from apache-beam[gcp]<3,>=2.47; python_version < \"3.11\"->tensorflow_data_validation[visualization]) (2.25.0)\n",
      "Requirement already satisfied: google-cloud-bigtable<3,>=2.19.0 in /opt/conda/lib/python3.10/site-packages (from apache-beam[gcp]<3,>=2.47; python_version < \"3.11\"->tensorflow_data_validation[visualization]) (2.23.1)\n",
      "Requirement already satisfied: google-cloud-spanner<4,>=3.0.0 in /opt/conda/lib/python3.10/site-packages (from apache-beam[gcp]<3,>=2.47; python_version < \"3.11\"->tensorflow_data_validation[visualization]) (3.46.0)\n",
      "Requirement already satisfied: google-cloud-dlp<4,>=3.0.0 in /opt/conda/lib/python3.10/site-packages (from apache-beam[gcp]<3,>=2.47; python_version < \"3.11\"->tensorflow_data_validation[visualization]) (3.18.0)\n",
      "Requirement already satisfied: google-cloud-language<3,>=2.0 in /opt/conda/lib/python3.10/site-packages (from apache-beam[gcp]<3,>=2.47; python_version < \"3.11\"->tensorflow_data_validation[visualization]) (2.13.3)\n",
      "Requirement already satisfied: google-cloud-videointelligence<3,>=2.0 in /opt/conda/lib/python3.10/site-packages (from apache-beam[gcp]<3,>=2.47; python_version < \"3.11\"->tensorflow_data_validation[visualization]) (2.13.3)\n",
      "Requirement already satisfied: google-cloud-vision<4,>=2 in /opt/conda/lib/python3.10/site-packages (from apache-beam[gcp]<3,>=2.47; python_version < \"3.11\"->tensorflow_data_validation[visualization]) (3.7.2)\n",
      "Requirement already satisfied: google-cloud-recommendations-ai<0.11.0,>=0.1.0 in /opt/conda/lib/python3.10/site-packages (from apache-beam[gcp]<3,>=2.47; python_version < \"3.11\"->tensorflow_data_validation[visualization]) (0.10.10)\n",
      "Requirement already satisfied: googleapis-common-protos<2.0.dev0,>=1.56.2 in /opt/conda/lib/python3.10/site-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.34.1->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.34.1->google-cloud-aiplatform) (1.63.0)\n",
      "Requirement already satisfied: grpcio-status<2.0.dev0,>=1.33.2 in /opt/conda/lib/python3.10/site-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.34.1->google-cloud-aiplatform) (1.48.2)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /opt/conda/lib/python3.10/site-packages (from google-auth<3.0.0dev,>=2.14.1->google-cloud-aiplatform) (0.4.0)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /opt/conda/lib/python3.10/site-packages (from google-auth<3.0.0dev,>=2.14.1->google-cloud-aiplatform) (4.9)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in /opt/conda/lib/python3.10/site-packages (from google-auth-oauthlib>=0.7.0->pandas_gbq) (2.0.0)\n",
      "Requirement already satisfied: grpc-google-iam-v1<1.0.0dev,>=0.12.4 in /opt/conda/lib/python3.10/site-packages (from google-cloud-resource-manager<3.0.0dev,>=1.3.3->google-cloud-aiplatform) (0.13.0)\n",
      "Requirement already satisfied: google-crc32c<2.0dev,>=1.0 in /opt/conda/lib/python3.10/site-packages (from google-cloud-storage<3.0.0dev,>=1.32.0->google-cloud-aiplatform) (1.5.0)\n",
      "Requirement already satisfied: jedi>=0.16 in /opt/conda/lib/python3.10/site-packages (from ipython<8,>=7->tensorflow_data_validation[visualization]) (0.19.1)\n",
      "Requirement already satisfied: decorator in /opt/conda/lib/python3.10/site-packages (from ipython<8,>=7->tensorflow_data_validation[visualization]) (5.1.1)\n",
      "Requirement already satisfied: pickleshare in /opt/conda/lib/python3.10/site-packages (from ipython<8,>=7->tensorflow_data_validation[visualization]) (0.7.5)\n",
      "Requirement already satisfied: traitlets>=4.2 in /opt/conda/lib/python3.10/site-packages (from ipython<8,>=7->tensorflow_data_validation[visualization]) (5.14.3)\n",
      "Requirement already satisfied: prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from ipython<8,>=7->tensorflow_data_validation[visualization]) (3.0.42)\n",
      "Requirement already satisfied: pygments in /opt/conda/lib/python3.10/site-packages (from ipython<8,>=7->tensorflow_data_validation[visualization]) (2.18.0)\n",
      "Requirement already satisfied: backcall in /opt/conda/lib/python3.10/site-packages (from ipython<8,>=7->tensorflow_data_validation[visualization]) (0.2.0)\n",
      "Requirement already satisfied: matplotlib-inline in /opt/conda/lib/python3.10/site-packages (from ipython<8,>=7->tensorflow_data_validation[visualization]) (0.1.7)\n",
      "Requirement already satisfied: pexpect>4.3 in /opt/conda/lib/python3.10/site-packages (from ipython<8,>=7->tensorflow_data_validation[visualization]) (4.9.0)\n",
      "Requirement already satisfied: certifi in /opt/conda/lib/python3.10/site-packages (from kfp-server-api<2.1.0,>=2.0.0->kfp) (2024.2.2)\n",
      "Requirement already satisfied: websocket-client!=0.40.0,!=0.41.*,!=0.42.*,>=0.32.0 in /opt/conda/lib/python3.10/site-packages (from kubernetes<27,>=8.0.0->kfp) (1.8.0)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in /opt/conda/lib/python3.10/site-packages (from pydantic<3->google-cloud-aiplatform) (0.6.0)\n",
      "Requirement already satisfied: pydantic-core==2.18.2 in /opt/conda/lib/python3.10/site-packages (from pydantic<3->google-cloud-aiplatform) (2.18.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests<3.0.0dev,>=2.21.0->google-cloud-bigquery) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests<3.0.0dev,>=2.21.0->google-cloud-bigquery) (3.7)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow<2.16,>=2.15->tensorflow_data_validation[visualization]) (1.6.3)\n",
      "Requirement already satisfied: flatbuffers>=23.5.26 in /opt/conda/lib/python3.10/site-packages (from tensorflow<2.16,>=2.15->tensorflow_data_validation[visualization]) (24.3.25)\n",
      "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /opt/conda/lib/python3.10/site-packages (from tensorflow<2.16,>=2.15->tensorflow_data_validation[visualization]) (0.6.0)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in /opt/conda/lib/python3.10/site-packages (from tensorflow<2.16,>=2.15->tensorflow_data_validation[visualization]) (0.2.0)\n",
      "Requirement already satisfied: h5py>=2.9.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow<2.16,>=2.15->tensorflow_data_validation[visualization]) (3.11.0)\n",
      "Requirement already satisfied: libclang>=13.0.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow<2.16,>=2.15->tensorflow_data_validation[visualization]) (18.1.1)\n",
      "Requirement already satisfied: ml-dtypes~=0.3.1 in /opt/conda/lib/python3.10/site-packages (from tensorflow<2.16,>=2.15->tensorflow_data_validation[visualization]) (0.3.2)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in /opt/conda/lib/python3.10/site-packages (from tensorflow<2.16,>=2.15->tensorflow_data_validation[visualization]) (3.3.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow<2.16,>=2.15->tensorflow_data_validation[visualization]) (2.4.0)\n",
      "Requirement already satisfied: wrapt<1.15,>=1.11.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow<2.16,>=2.15->tensorflow_data_validation[visualization]) (1.14.1)\n",
      "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /opt/conda/lib/python3.10/site-packages (from tensorflow<2.16,>=2.15->tensorflow_data_validation[visualization]) (0.37.0)\n",
      "Requirement already satisfied: tensorboard<2.16,>=2.15 in /opt/conda/lib/python3.10/site-packages (from tensorflow<2.16,>=2.15->tensorflow_data_validation[visualization]) (2.15.2)\n",
      "Requirement already satisfied: tensorflow-estimator<2.16,>=2.15.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow<2.16,>=2.15->tensorflow_data_validation[visualization]) (2.15.0)\n",
      "Requirement already satisfied: keras<2.16,>=2.15.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow<2.16,>=2.15->tensorflow_data_validation[visualization]) (2.15.0)\n",
      "INFO: pip is looking at multiple versions of tensorflow-metadata to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting tensorflow<2.16,>=2.15 (from tensorflow_data_validation[visualization])\n",
      "  Using cached tensorflow-2.15.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.2 kB)\n",
      "  Using cached tensorflow-2.15.0.post1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.2 kB)\n",
      "Collecting ml-dtypes~=0.2.0 (from tensorflow<2.16,>=2.15->tensorflow_data_validation[visualization])\n",
      "  Using cached ml_dtypes-0.2.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (20 kB)\n",
      "Collecting tensorflow<2.16,>=2.15 (from tensorflow_data_validation[visualization])\n",
      "  Using cached tensorflow-2.15.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.4 kB)\n",
      "Collecting proto-plus<2.0.0dev,>=1.22.0 (from google-cloud-aiplatform)\n",
      "  Using cached proto_plus-1.24.0-py3-none-any.whl.metadata (2.2 kB)\n",
      "INFO: pip is still looking at multiple versions of tensorflow-metadata to determine which version is compatible with other requirements. This could take a while.\n",
      "  Using cached proto_plus-1.23.0-py3-none-any.whl.metadata (2.2 kB)\n",
      "  Using cached proto_plus-1.22.3-py3-none-any.whl.metadata (2.2 kB)\n",
      "INFO: This is taking longer than usual. You might need to provide the dependency resolver with stricter constraints to reduce runtime. See https://pip.pypa.io/warnings/backtracking for guidance. If you want to abort this run, press Ctrl + C.\n",
      "Collecting google-cloud-storage<3.0.0dev,>=1.32.0 (from google-cloud-aiplatform)\n",
      "  Using cached google_cloud_storage-2.17.0-py2.py3-none-any.whl.metadata (6.6 kB)\n",
      "Collecting google-cloud-resource-manager<3.0.0dev,>=1.3.3 (from google-cloud-aiplatform)\n",
      "  Using cached google_cloud_resource_manager-1.12.3-py2.py3-none-any.whl.metadata (5.3 kB)\n",
      "  Using cached google_cloud_resource_manager-1.12.2-py2.py3-none-any.whl.metadata (5.3 kB)\n",
      "  Using cached google_cloud_resource_manager-1.12.1-py2.py3-none-any.whl.metadata (5.3 kB)\n",
      "  Using cached google_cloud_resource_manager-1.12.0-py2.py3-none-any.whl.metadata (5.2 kB)\n",
      "  Using cached google_cloud_resource_manager-1.11.0-py2.py3-none-any.whl.metadata (5.2 kB)\n",
      "  Using cached google_cloud_resource_manager-1.10.4-py2.py3-none-any.whl.metadata (5.2 kB)\n",
      "  Using cached google_cloud_resource_manager-1.10.3-py2.py3-none-any.whl.metadata (5.2 kB)\n",
      "  Using cached google_cloud_resource_manager-1.10.2-py2.py3-none-any.whl.metadata (5.1 kB)\n",
      "  Using cached google_cloud_resource_manager-1.10.1-py2.py3-none-any.whl.metadata (5.2 kB)\n",
      "  Using cached google_cloud_resource_manager-1.10.0-py2.py3-none-any.whl.metadata (5.2 kB)\n",
      "  Using cached google_cloud_resource_manager-1.9.1-py2.py3-none-any.whl.metadata (5.2 kB)\n",
      "  Using cached google_cloud_resource_manager-1.9.0-py2.py3-none-any.whl.metadata (5.2 kB)\n",
      "  Using cached google_cloud_resource_manager-1.8.1-py2.py3-none-any.whl.metadata (5.2 kB)\n",
      "  Using cached google_cloud_resource_manager-1.8.0-py2.py3-none-any.whl.metadata (5.2 kB)\n",
      "  Using cached google_cloud_resource_manager-1.7.0-py2.py3-none-any.whl.metadata (5.0 kB)\n",
      "  Using cached google_cloud_resource_manager-1.6.3-py2.py3-none-any.whl.metadata (5.0 kB)\n",
      "  Using cached google_cloud_resource_manager-1.6.2-py2.py3-none-any.whl.metadata (4.9 kB)\n",
      "  Using cached google_cloud_resource_manager-1.6.1-py2.py3-none-any.whl.metadata (4.9 kB)\n",
      "  Using cached google_cloud_resource_manager-1.6.0-py2.py3-none-any.whl.metadata (4.9 kB)\n",
      "INFO: pip is looking at multiple versions of google-cloud-resource-manager to determine which version is compatible with other requirements. This could take a while.\n",
      "  Using cached google_cloud_resource_manager-1.5.1-py2.py3-none-any.whl.metadata (4.3 kB)\n",
      "  Using cached google_cloud_resource_manager-1.5.0-py2.py3-none-any.whl.metadata (4.3 kB)\n",
      "Collecting google-cloud-core<3.0.0dev,>=1.6.0 (from google-cloud-bigquery)\n",
      "  Using cached google_cloud_core-2.4.1-py2.py3-none-any.whl.metadata (2.7 kB)\n",
      "Collecting google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.34.1 (from google-cloud-aiplatform)\n",
      "  Using cached google_api_core-2.19.1-py3-none-any.whl.metadata (2.7 kB)\n",
      "  Using cached google_api_core-2.19.0-py3-none-any.whl.metadata (2.7 kB)\n",
      "  Using cached google_api_core-2.18.0-py3-none-any.whl.metadata (2.7 kB)\n",
      "  Using cached google_api_core-2.17.1-py3-none-any.whl.metadata (2.7 kB)\n",
      "Collecting proto-plus<2.0.0dev,>=1.22.0 (from google-cloud-aiplatform)\n",
      "  Using cached proto_plus-1.22.2-py3-none-any.whl.metadata (2.2 kB)\n",
      "  Using cached proto_plus-1.22.1-py3-none-any.whl.metadata (2.1 kB)\n",
      "  Using cached proto_plus-1.22.0-py3-none-any.whl.metadata (2.1 kB)\n",
      "Collecting google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.34.1 (from google-cloud-aiplatform)\n",
      "  Using cached google_api_core-2.17.0-py3-none-any.whl.metadata (2.7 kB)\n",
      "  Using cached google_api_core-2.16.2-py3-none-any.whl.metadata (2.7 kB)\n",
      "  Using cached google_api_core-2.16.1-py3-none-any.whl.metadata (2.7 kB)\n",
      "  Using cached google_api_core-2.16.0-py3-none-any.whl.metadata (2.7 kB)\n",
      "  Using cached google_api_core-2.15.0-py3-none-any.whl.metadata (2.7 kB)\n",
      "  Using cached google_api_core-2.14.0-py3-none-any.whl.metadata (2.6 kB)\n",
      "  Using cached google_api_core-2.13.1-py3-none-any.whl.metadata (2.6 kB)\n",
      "  Using cached google_api_core-2.13.0-py3-none-any.whl.metadata (2.7 kB)\n",
      "  Using cached google_api_core-2.12.0-py3-none-any.whl.metadata (2.7 kB)\n",
      "  Using cached google_api_core-2.11.1-py3-none-any.whl.metadata (2.7 kB)\n",
      "  Using cached google_api_core-2.11.0-py3-none-any.whl.metadata (2.6 kB)\n",
      "INFO: pip is looking at multiple versions of google-api-core[grpc] to determine which version is compatible with other requirements. This could take a while.\n",
      "INFO: pip is still looking at multiple versions of google-api-core[grpc] to determine which version is compatible with other requirements. This could take a while.\n",
      "INFO: This is taking longer than usual. You might need to provide the dependency resolver with stricter constraints to reduce runtime. See https://pip.pypa.io/warnings/backtracking for guidance. If you want to abort this run, press Ctrl + C.\n",
      "Collecting apache-beam[gcp]<3,>=2.47 (from tensorflow_data_validation[visualization])\n",
      "  Using cached apache_beam-2.57.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (7.6 kB)\n",
      "Requirement already satisfied: jsonschema<5.0.0,>=4.0.0 in /opt/conda/lib/python3.10/site-packages (from apache-beam[gcp]<3,>=2.47; python_version < \"3.11\"->tensorflow_data_validation[visualization]) (4.22.0)\n",
      "Collecting jsonpickle<4.0.0,>=3.0.0 (from apache-beam[gcp]<3,>=2.47; python_version < \"3.11\"->tensorflow_data_validation[visualization])\n",
      "  Using cached jsonpickle-3.2.2-py3-none-any.whl.metadata (7.2 kB)\n",
      "Collecting protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5 (from google-cloud-aiplatform)\n",
      "  Using cached protobuf-4.22.5-cp37-abi3-manylinux2014_x86_64.whl.metadata (540 bytes)\n",
      "Collecting redis<6,>=5.0.0 (from apache-beam[gcp]<3,>=2.47; python_version < \"3.11\"->tensorflow_data_validation[visualization])\n",
      "  Using cached redis-5.0.7-py3-none-any.whl.metadata (9.3 kB)\n",
      "Collecting pyarrow-hotfix<1 (from apache-beam[gcp]<3,>=2.47; python_version < \"3.11\"->tensorflow_data_validation[visualization])\n",
      "  Using cached pyarrow_hotfix-0.6-py3-none-any.whl.metadata (3.6 kB)\n",
      "Collecting js2py<1,>=0.74 (from apache-beam[gcp]<3,>=2.47; python_version < \"3.11\"->tensorflow_data_validation[visualization])\n",
      "  Using cached Js2Py-0.74-py3-none-any.whl.metadata (868 bytes)\n",
      "Collecting google-cloud-storage<3.0.0dev,>=1.32.0 (from google-cloud-aiplatform)\n",
      "  Using cached google_cloud_storage-2.16.0-py2.py3-none-any.whl.metadata (6.1 kB)\n",
      "Collecting apache-beam[gcp]<3,>=2.47 (from tensorflow_data_validation[visualization])\n",
      "  Using cached apache_beam-2.56.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\n",
      "  Using cached apache_beam-2.55.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.6 kB)\n",
      "INFO: pip is still looking at multiple versions of google-cloud-resource-manager to determine which version is compatible with other requirements. This could take a while.\n",
      "  Using cached apache_beam-2.55.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.6 kB)\n",
      "  Using cached apache_beam-2.54.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.6 kB)\n",
      "  Using cached apache_beam-2.53.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.6 kB)\n",
      "INFO: This is taking longer than usual. You might need to provide the dependency resolver with stricter constraints to reduce runtime. See https://pip.pypa.io/warnings/backtracking for guidance. If you want to abort this run, press Ctrl + C.\n",
      "  Using cached apache_beam-2.52.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.3 kB)\n",
      "  Using cached apache_beam-2.51.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.1 kB)\n",
      "Collecting protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5 (from google-cloud-aiplatform)\n",
      "  Using cached protobuf-4.24.4-cp37-abi3-manylinux2014_x86_64.whl.metadata (540 bytes)\n",
      "Collecting apache-beam[gcp]<3,>=2.47 (from tensorflow_data_validation[visualization])\n",
      "  Using cached apache_beam-2.50.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.0 kB)\n",
      "  Using cached apache_beam-2.49.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.9 kB)\n",
      "  Using cached apache_beam-2.48.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.2 kB)\n",
      "Collecting google-cloud-bigtable<2.18.0,>=2.0.0 (from apache-beam[gcp]<3,>=2.47; python_version < \"3.11\"->tensorflow_data_validation[visualization])\n",
      "  Using cached google_cloud_bigtable-2.17.0-py2.py3-none-any.whl.metadata (5.1 kB)\n",
      "Collecting apache-beam[gcp]<3,>=2.47 (from tensorflow_data_validation[visualization])\n",
      "  Using cached apache_beam-2.47.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.0 kB)\n",
      "Collecting httplib2<0.23.0,>=0.8 (from apache-beam<3,>=2.47->apache-beam[gcp]<3,>=2.47; python_version < \"3.11\"->tensorflow_data_validation[visualization])\n",
      "  Using cached httplib2-0.21.0-py3-none-any.whl.metadata (2.6 kB)\n",
      "Collecting pandas_gbq\n",
      "  Using cached pandas_gbq-0.23.1-py2.py3-none-any.whl.metadata (3.4 kB)\n",
      "INFO: pip is looking at multiple versions of apache-beam[gcp] to determine which version is compatible with other requirements. This could take a while.\n",
      "INFO: pip is still looking at multiple versions of apache-beam[gcp] to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting tensorflow_data_validation[visualization]\n",
      "  Using cached tensorflow_data_validation-1.15.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (18 kB)\n",
      "INFO: This is taking longer than usual. You might need to provide the dependency resolver with stricter constraints to reduce runtime. See https://pip.pypa.io/warnings/backtracking for guidance. If you want to abort this run, press Ctrl + C.\n",
      "  Downloading tensorflow_data_validation-1.15.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (18 kB)\n",
      "Collecting tensorflow~=2.15 (from tensorflow_data_validation[visualization])\n",
      "  Downloading tensorflow-2.16.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.2 kB)\n",
      "Collecting tensorboard<2.17,>=2.16 (from tensorflow~=2.15->tensorflow_data_validation[visualization])\n",
      "  Using cached tensorboard-2.16.2-py3-none-any.whl.metadata (1.6 kB)\n",
      "Collecting keras>=3.0.0 (from tensorflow~=2.15->tensorflow_data_validation[visualization])\n",
      "  Using cached keras-3.4.1-py3-none-any.whl.metadata (5.8 kB)\n",
      "Collecting tensorflow~=2.15 (from tensorflow_data_validation[visualization])\n",
      "  Downloading tensorflow-2.16.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.3 kB)\n",
      "Collecting tensorflow_data_validation[visualization]\n",
      "  Downloading tensorflow_data_validation-1.14.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (18 kB)\n",
      "Collecting tensorflow-metadata<1.15,>=1.14.0 (from tensorflow_data_validation[visualization])\n",
      "  Downloading tensorflow_metadata-1.14.0-py3-none-any.whl.metadata (2.1 kB)\n",
      "Collecting tfx-bsl<1.15,>=1.14.0 (from tensorflow_data_validation[visualization])\n",
      "  Downloading tfx_bsl-1.14.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (10 kB)\n",
      "Collecting tensorflow<3,>=2.13 (from tensorflow_data_validation[visualization])\n",
      "  Downloading tensorflow-2.14.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.1 kB)\n",
      "Collecting tensorboard<2.15,>=2.14 (from tensorflow<3,>=2.13->tensorflow_data_validation[visualization])\n",
      "  Downloading tensorboard-2.14.1-py3-none-any.whl.metadata (1.7 kB)\n",
      "Collecting tensorflow-estimator<2.15,>=2.14.0 (from tensorflow<3,>=2.13->tensorflow_data_validation[visualization])\n",
      "  Downloading tensorflow_estimator-2.14.0-py2.py3-none-any.whl.metadata (1.3 kB)\n",
      "Collecting keras<2.15,>=2.14.0 (from tensorflow<3,>=2.13->tensorflow_data_validation[visualization])\n",
      "  Downloading keras-2.14.0-py3-none-any.whl.metadata (2.4 kB)\n",
      "Collecting tensorflow<3,>=2.13 (from tensorflow_data_validation[visualization])\n",
      "  Downloading tensorflow-2.14.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.1 kB)\n",
      "  Downloading tensorflow-2.13.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.4 kB)\n",
      "Collecting gast<=0.4.0,>=0.2.1 (from tensorflow<3,>=2.13->tensorflow_data_validation[visualization])\n",
      "  Downloading gast-0.4.0-py3-none-any.whl.metadata (1.1 kB)\n",
      "Collecting keras<2.14,>=2.13.1 (from tensorflow<3,>=2.13->tensorflow_data_validation[visualization])\n",
      "  Downloading keras-2.13.1-py3-none-any.whl.metadata (2.4 kB)\n",
      "Collecting numpy>=1.16.6 (from pandas_gbq)\n",
      "  Downloading numpy-1.24.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.6 kB)\n",
      "Collecting tensorboard<2.14,>=2.13 (from tensorflow<3,>=2.13->tensorflow_data_validation[visualization])\n",
      "  Downloading tensorboard-2.13.0-py3-none-any.whl.metadata (1.8 kB)\n",
      "Collecting tensorflow-estimator<2.14,>=2.13.0 (from tensorflow<3,>=2.13->tensorflow_data_validation[visualization])\n",
      "  Downloading tensorflow_estimator-2.13.0-py2.py3-none-any.whl.metadata (1.3 kB)\n",
      "INFO: pip is looking at multiple versions of tensorflow to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting tensorflow<3,>=2.13 (from tensorflow_data_validation[visualization])\n",
      "  Downloading tensorflow-2.13.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.4 kB)\n",
      "Collecting pydantic-core==2.18.2 (from pydantic<3->google-cloud-aiplatform)\n",
      "  Downloading pydantic_core-2.18.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.5 kB)\n",
      "Collecting pydantic<3 (from google-cloud-aiplatform)\n",
      "  Downloading pydantic-2.8.2-py3-none-any.whl.metadata (125 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m125.2/125.2 kB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting pydantic-core==2.20.1 (from pydantic<3->google-cloud-aiplatform)\n",
      "  Downloading pydantic_core-2.20.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.6 kB)\n",
      "Collecting pydantic<3 (from google-cloud-aiplatform)\n",
      "  Downloading pydantic-2.8.1-py3-none-any.whl.metadata (124 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m124.5/124.5 kB\u001b[0m \u001b[31m7.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hINFO: pip is still looking at multiple versions of tensorflow to determine which version is compatible with other requirements. This could take a while.\n",
      "  Downloading pydantic-2.8.0-py3-none-any.whl.metadata (123 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m123.5/123.5 kB\u001b[0m \u001b[31m11.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting pydantic-core==2.20.0 (from pydantic<3->google-cloud-aiplatform)\n",
      "  Downloading pydantic_core-2.20.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.6 kB)\n",
      "Collecting pydantic<3 (from google-cloud-aiplatform)\n",
      "  Using cached pydantic-2.7.4-py3-none-any.whl.metadata (109 kB)\n",
      "Collecting pydantic-core==2.18.4 (from pydantic<3->google-cloud-aiplatform)\n",
      "  Downloading pydantic_core-2.18.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.5 kB)\n",
      "Collecting pydantic<3 (from google-cloud-aiplatform)\n",
      "  Downloading pydantic-2.7.3-py3-none-any.whl.metadata (108 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m109.0/109.0 kB\u001b[0m \u001b[31m9.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hINFO: This is taking longer than usual. You might need to provide the dependency resolver with stricter constraints to reduce runtime. See https://pip.pypa.io/warnings/backtracking for guidance. If you want to abort this run, press Ctrl + C.\n",
      "  Downloading pydantic-2.7.2-py3-none-any.whl.metadata (108 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m108.5/108.5 kB\u001b[0m \u001b[31m10.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting pydantic-core==2.18.3 (from pydantic<3->google-cloud-aiplatform)\n",
      "  Downloading pydantic_core-2.18.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.5 kB)\n",
      "Collecting pydantic<3 (from google-cloud-aiplatform)\n",
      "  Downloading pydantic-2.7.1-py3-none-any.whl.metadata (107 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m107.3/107.3 kB\u001b[0m \u001b[31m10.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Downloading pydantic-2.7.0-py3-none-any.whl.metadata (103 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m103.4/103.4 kB\u001b[0m \u001b[31m10.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting pydantic-core==2.18.1 (from pydantic<3->google-cloud-aiplatform)\n",
      "  Downloading pydantic_core-2.18.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.5 kB)\n",
      "Collecting pydantic<3 (from google-cloud-aiplatform)\n",
      "  Downloading pydantic-2.6.4-py3-none-any.whl.metadata (85 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m85.1/85.1 kB\u001b[0m \u001b[31m9.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting pydantic-core==2.16.3 (from pydantic<3->google-cloud-aiplatform)\n",
      "  Downloading pydantic_core-2.16.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.5 kB)\n",
      "Collecting pydantic<3 (from google-cloud-aiplatform)\n",
      "  Downloading pydantic-2.6.3-py3-none-any.whl.metadata (84 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.4/84.4 kB\u001b[0m \u001b[31m9.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Downloading pydantic-2.6.2-py3-none-any.whl.metadata (83 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.0/84.0 kB\u001b[0m \u001b[31m10.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Downloading pydantic-2.6.1-py3-none-any.whl.metadata (83 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m83.5/83.5 kB\u001b[0m \u001b[31m7.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting pydantic-core==2.16.2 (from pydantic<3->google-cloud-aiplatform)\n",
      "  Downloading pydantic_core-2.16.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.5 kB)\n",
      "Collecting pydantic<3 (from google-cloud-aiplatform)\n",
      "  Downloading pydantic-2.6.0-py3-none-any.whl.metadata (81 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m81.8/81.8 kB\u001b[0m \u001b[31m7.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting pydantic-core==2.16.1 (from pydantic<3->google-cloud-aiplatform)\n",
      "  Downloading pydantic_core-2.16.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.5 kB)\n",
      "Collecting pydantic<3 (from google-cloud-aiplatform)\n",
      "  Downloading pydantic-2.5.3-py3-none-any.whl.metadata (65 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m65.6/65.6 kB\u001b[0m \u001b[31m6.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting pydantic-core==2.14.6 (from pydantic<3->google-cloud-aiplatform)\n",
      "  Downloading pydantic_core-2.14.6-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.5 kB)\n",
      "Collecting pydantic<3 (from google-cloud-aiplatform)\n",
      "  Downloading pydantic-2.5.2-py3-none-any.whl.metadata (65 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m65.2/65.2 kB\u001b[0m \u001b[31m6.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting pydantic-core==2.14.5 (from pydantic<3->google-cloud-aiplatform)\n",
      "  Downloading pydantic_core-2.14.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.5 kB)\n",
      "Collecting pydantic<3 (from google-cloud-aiplatform)\n",
      "  Downloading pydantic-2.5.1-py3-none-any.whl.metadata (64 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m64.1/64.1 kB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting pydantic-core==2.14.3 (from pydantic<3->google-cloud-aiplatform)\n",
      "  Downloading pydantic_core-2.14.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.5 kB)\n",
      "Collecting pydantic<3 (from google-cloud-aiplatform)\n",
      "  Downloading pydantic-2.5.0-py3-none-any.whl.metadata (174 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m174.6/174.6 kB\u001b[0m \u001b[31m11.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting pydantic-core==2.14.1 (from pydantic<3->google-cloud-aiplatform)\n",
      "  Downloading pydantic_core-2.14.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.5 kB)\n",
      "Collecting pydantic<3 (from google-cloud-aiplatform)\n",
      "  Downloading pydantic-2.4.2-py3-none-any.whl.metadata (158 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m158.6/158.6 kB\u001b[0m \u001b[31m11.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting pydantic-core==2.10.1 (from pydantic<3->google-cloud-aiplatform)\n",
      "  Downloading pydantic_core-2.10.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.5 kB)\n",
      "Collecting pydantic<3 (from google-cloud-aiplatform)\n",
      "  Downloading pydantic-2.4.1-py3-none-any.whl.metadata (157 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m157.1/157.1 kB\u001b[0m \u001b[31m12.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Downloading pydantic-2.4.0-py3-none-any.whl.metadata (156 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m156.4/156.4 kB\u001b[0m \u001b[31m14.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting pydantic-core==2.10.0 (from pydantic<3->google-cloud-aiplatform)\n",
      "  Downloading pydantic_core-2.10.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.5 kB)\n",
      "Collecting pydantic<3 (from google-cloud-aiplatform)\n",
      "  Downloading pydantic-2.3.0-py3-none-any.whl.metadata (148 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m148.8/148.8 kB\u001b[0m \u001b[31m13.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting pydantic-core==2.6.3 (from pydantic<3->google-cloud-aiplatform)\n",
      "  Downloading pydantic_core-2.6.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.5 kB)\n",
      "Collecting pydantic<3 (from google-cloud-aiplatform)\n",
      "  Downloading pydantic-2.2.1-py3-none-any.whl.metadata (145 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m145.6/145.6 kB\u001b[0m \u001b[31m14.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting pydantic-core==2.6.1 (from pydantic<3->google-cloud-aiplatform)\n",
      "  Downloading pydantic_core-2.6.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.5 kB)\n",
      "Collecting pydantic<3 (from google-cloud-aiplatform)\n",
      "  Downloading pydantic-2.2.0-py3-none-any.whl.metadata (145 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m145.6/145.6 kB\u001b[0m \u001b[31m11.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting pydantic-core==2.6.0 (from pydantic<3->google-cloud-aiplatform)\n",
      "  Downloading pydantic_core-2.6.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.5 kB)\n",
      "Collecting pydantic<3 (from google-cloud-aiplatform)\n",
      "  Downloading pydantic-2.1.1-py3-none-any.whl.metadata (136 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m136.5/136.5 kB\u001b[0m \u001b[31m12.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting pydantic-core==2.4.0 (from pydantic<3->google-cloud-aiplatform)\n",
      "  Downloading pydantic_core-2.4.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.5 kB)\n",
      "Collecting pydantic<3 (from google-cloud-aiplatform)\n",
      "  Downloading pydantic-2.1.0-py3-none-any.whl.metadata (136 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m136.2/136.2 kB\u001b[0m \u001b[31m12.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Downloading pydantic-2.0.3-py3-none-any.whl.metadata (128 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m128.2/128.2 kB\u001b[0m \u001b[31m11.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting pydantic-core==2.3.0 (from pydantic<3->google-cloud-aiplatform)\n",
      "  Downloading pydantic_core-2.3.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.1 kB)\n",
      "Collecting pydantic<3 (from google-cloud-aiplatform)\n",
      "  Downloading pydantic-2.0.2-py3-none-any.whl.metadata (119 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m120.0/120.0 kB\u001b[0m \u001b[31m12.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting pydantic-core==2.1.2 (from pydantic<3->google-cloud-aiplatform)\n",
      "  Downloading pydantic_core-2.1.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.1 kB)\n",
      "Collecting pydantic<3 (from google-cloud-aiplatform)\n",
      "  Downloading pydantic-2.0.1-py3-none-any.whl.metadata (119 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m119.0/119.0 kB\u001b[0m \u001b[31m10.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting pydantic-core==2.0.2 (from pydantic<3->google-cloud-aiplatform)\n",
      "  Downloading pydantic_core-2.0.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.1 kB)\n",
      "Collecting pydantic<3 (from google-cloud-aiplatform)\n",
      "  Downloading pydantic-2.0-py3-none-any.whl.metadata (117 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m118.0/118.0 kB\u001b[0m \u001b[31m12.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting pydantic-core==2.0.1 (from pydantic<3->google-cloud-aiplatform)\n",
      "  Downloading pydantic_core-2.0.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.2 kB)\n",
      "Collecting pydantic<3 (from google-cloud-aiplatform)\n",
      "  Downloading pydantic-1.10.17-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (151 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m151.6/151.6 kB\u001b[0m \u001b[31m14.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting typing-extensions>=3.7.0 (from apache-beam<3,>=2.47->apache-beam[gcp]<3,>=2.47; python_version < \"3.11\"->tensorflow_data_validation[visualization])\n",
      "  Downloading typing_extensions-4.5.0-py3-none-any.whl.metadata (8.5 kB)\n",
      "\u001b[31mERROR: Cannot install apache-beam[gcp]==2.50.0 and tensorflow-data-validation[visualization]==1.14.0 because these package versions have conflicting dependencies.\u001b[0m\u001b[31m\n",
      "\u001b[0m\n",
      "The conflict is caused by:\n",
      "    tensorflow-data-validation[visualization] 1.14.0 depends on apache-beam<3 and >=2.47\n",
      "    apache-beam[gcp] 2.50.0 depends on apache-beam 2.50.0 (Installed)\n",
      "\n",
      "To fix this you could try to:\n",
      "1. loosen the range of package versions you've specified\n",
      "2. remove package versions to allow pip attempt to solve the dependency conflict\n",
      "\n",
      "\u001b[31mERROR: ResolutionImpossible: for help visit https://pip.pypa.io/en/latest/topics/dependency-resolution/#dealing-with-dependency-conflicts\u001b[0m\u001b[31m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "! pip3 install --upgrade \\\n",
    "    google-cloud-aiplatform \\\n",
    "    google-cloud-bigquery \\\n",
    "    kfp \\\n",
    "    pandas \\\n",
    "    pandas_gbq \\\n",
    "    pyarrow \\\n",
    "    tensorflow_data_validation[visualization]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env_var:\n",
      "env:\n",
      "global:\n",
      "  /etc/xdg/pip/pip.conf, exists: False\n",
      "  /etc/pip.conf, exists: False\n",
      "site:\n",
      "  /opt/dataproc/conda/pip.conf, exists: False\n",
      "user:\n",
      "  /home/spark/.pip/pip.conf, exists: True\n",
      "    global.index-url: https://us-python.pkg.dev/artifact-registry-python-cache/virtual-python/simple/\n",
      "    global.target: /mnt/dataproc/python/site-packages\n",
      "  /home/spark/.config/pip/pip.conf, exists: False\n"
     ]
    }
   ],
   "source": [
    "!pip config debug\n",
    "# !python --version\n",
    "# !pip install --upgrade pip setuptools wheel\n",
    "# !unset PIP_TARGET\n",
    "\n",
    "# !pip show google-cloud-aiplatform\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Channels:\n",
      " - conda-forge\n",
      "Platform: linux-64\n",
      "Collecting package metadata (repodata.json): / Retrying (Retry(total=2, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x7fea0e1c08c0>: Failed to establish a new connection: [Errno 101] Network is unreachable')': /conda-forge/noarch/repodata.json.zst\n",
      "\n",
      "Retrying (Retry(total=2, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x7fea0f8ace60>: Failed to establish a new connection: [Errno 101] Network is unreachable')': /conda-forge/linux-64/repodata.json.zst\n",
      "\n",
      "| Retrying (Retry(total=1, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x7fea0e1c0aa0>: Failed to establish a new connection: [Errno 101] Network is unreachable')': /conda-forge/linux-64/repodata.json.zst\n",
      "\n",
      "Retrying (Retry(total=1, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x7fea0e1c0920>: Failed to establish a new connection: [Errno 101] Network is unreachable')': /conda-forge/noarch/repodata.json.zst\n",
      "\n",
      "/ ^C\n",
      "| \n",
      "CondaError: Run 'conda init' before 'conda activate'\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# !conda --version\n",
    "# !conda create -n myenv python=3.9 numpy pandas matplotlib\n",
    "# !conda init \n",
    "# !conda activate myenv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "02nrfqgSOsw2"
   },
   "source": [
    "Check that the version of google-cloud-aiplatform is 1.51.0 or later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "uq2kSzjTOuQp"
   },
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'aiplatform' from 'google.cloud' (unknown location)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mgoogle\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcloud\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m aiplatform\n\u001b[1;32m      3\u001b[0m aiplatform\u001b[38;5;241m.\u001b[39m__version__\n",
      "\u001b[0;31mImportError\u001b[0m: cannot import name 'aiplatform' from 'google.cloud' (unknown location)"
     ]
    }
   ],
   "source": [
    "from google.cloud import aiplatform\n",
    "\n",
    "aiplatform.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "U4o9GjHNfHdI"
   },
   "source": [
    "### Restart runtime (Colab only)\n",
    "\n",
    "To use the newly installed packages, you must restart the runtime on Google Colab."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JaY8Q_T3fA38"
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "if \"google.colab\" in sys.modules:\n",
    "\n",
    "    import IPython\n",
    "\n",
    "    app = IPython.Application.instance()\n",
    "    app.kernel.do_shutdown(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4LvVXIhRO0Vk"
   },
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "<b>⚠️ The kernel is going to restart. Please wait until it is finished before continuing to the next step. ⚠️</b>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "i5-StiZvO3-6"
   },
   "source": [
    "### Authenticate your notebook environment (Colab only)\n",
    "\n",
    "Authenticate your environment on Google Colab."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YIXcOK0uO5q_"
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "if \"google.colab\" in sys.modules:\n",
    "\n",
    "    from google.colab import auth\n",
    "\n",
    "    auth.authenticate_user()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uHVx1c-OO7KR"
   },
   "source": [
    "### Set Google Cloud project information and initialize Vertex AI SDK\n",
    "\n",
    "To get started using Vertex AI, you must have an existing Google Cloud project and [enable the Vertex AI API](https://console.cloud.google.com/flows/enableapi?apiid=aiplatform.googleapis.com). Learn more about [setting up a project and a development environment](https://cloud.google.com/vertex-ai/docs/start/cloud-environment)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "gg6xkyR7O9cU"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated property [core/project].\n",
      "Updated property [ai/region].\n"
     ]
    }
   ],
   "source": [
    "PROJECT_ID = \"vertex-ai-382806\"  # @param {type:\"string\"}\n",
    "LOCATION = \"us-central1\"  # @param {type:\"string\"}\n",
    "\n",
    "\n",
    "import os\n",
    "\n",
    "import vertexai\n",
    "\n",
    "! gcloud config set project $PROJECT_ID\n",
    "os.environ[\"GOOGLE_CLOUD_PROJECT\"] = PROJECT_ID\n",
    "! gcloud config set ai/region $LOCATION\n",
    "\n",
    "vertexai.init(project=PROJECT_ID, location=LOCATION)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LMRdCWwqPBpR"
   },
   "source": [
    "## Start Model Monitoring tutorial"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "l_BdNCfk7Hjz"
   },
   "source": [
    "### Step 1: Create a Cloud Storage bucket\n",
    "\n",
    "Create a Cloud Storage bucket to store intermediate artifacts such as datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "3_CZRJ5L60Mi"
   },
   "outputs": [],
   "source": [
    "# Create a Cloud Storage bucket\n",
    "BUCKET_URI = f\"gs://your-bucket-name-{PROJECT_ID}-unique\"  # @param {type:\"string\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lvHsyHsi7Dg8"
   },
   "source": [
    "**Only if your bucket doesn't already exist**: Run the following cell to create your Cloud Storage bucket."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "mrHlJi-e7AyX"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating gs://your-bucket-name-vertex-ai-382806-unique/...\n"
     ]
    }
   ],
   "source": [
    "! gsutil mb -l {LOCATION} -p {PROJECT_ID} {BUCKET_URI}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5ZLxDjB6lYXx"
   },
   "source": [
    "### Step 2: Prepare a model in Vertex AI Model Registry"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1_OchKcp60FE"
   },
   "source": [
    "You can register a model in Vertex AI Model Registry with its artifacts, enabling you to perform online serving or batch prediction. Alternatively, you can register a referenced/placeholder model that includes only the model's name.\n",
    "In this notebook, you register a model with artifacts because you'll run a batch prediction job."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "SwrD1CnmmJsk"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating Model\n",
      "Create Model backing LRO: projects/433578906282/locations/us-central1/models/133077190844612608/operations/5875056524061048832\n",
      "Model created. Resource name: projects/433578906282/locations/us-central1/models/133077190844612608@1\n",
      "To use this Model in another session:\n",
      "model = aiplatform.Model('projects/433578906282/locations/us-central1/models/133077190844612608@1')\n"
     ]
    }
   ],
   "source": [
    "import google.cloud.aiplatform as aiplatform\n",
    "\n",
    "MODEL_PATH = \"gs://mco-mm/churn\"\n",
    "MODEL_NAME = \"churn\"\n",
    "IMAGE = \"us-docker.pkg.dev/cloud-aiplatform/prediction/tf2-cpu.2-5:latest\"\n",
    "\n",
    "model = aiplatform.Model.upload(\n",
    "    display_name=MODEL_NAME,\n",
    "    artifact_uri=MODEL_PATH,\n",
    "    serving_container_image_uri=IMAGE,\n",
    "    sync=True,\n",
    ")\n",
    "\n",
    "MODEL_ID = model.resource_name.split(\"/\")[-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RuwAoHTilv1j"
   },
   "source": [
    "### Step 3: Create a model monitor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Wp6jVFGvC6wt"
   },
   "source": [
    "Create a model monitor to associate monitoring details with a model version that has been register in Vertex AI Model Registry."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9hH4wSCXfgoE"
   },
   "source": [
    "#### Define Model Monitoring Schema"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jyg0cMAdfrAa"
   },
   "source": [
    "The monitoring schema is required for model monitors. It includes the names of input features, prediction outputs and, if available, ground truths, along with their respective data type.\n",
    "\n",
    "**Note: For AutoML tables (regression and classification), defining the schema is optional. The schema is automatically fetched when available. If Vertex AI cannot get the schema information, you must provide it.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "GR-4eTn3fkHk"
   },
   "outputs": [],
   "source": [
    "from vertexai.resources.preview import ml_monitoring\n",
    "\n",
    "MODEL_MONITORING_SCHEMA = ml_monitoring.spec.ModelMonitoringSchema(\n",
    "    feature_fields=[\n",
    "        ml_monitoring.spec.FieldSchema(name=\"user_pseudo_id\", data_type=\"string\"),\n",
    "        ml_monitoring.spec.FieldSchema(name=\"country\", data_type=\"string\"),\n",
    "        ml_monitoring.spec.FieldSchema(name=\"operating_system\", data_type=\"string\"),\n",
    "        ml_monitoring.spec.FieldSchema(name=\"cnt_user_engagement\", data_type=\"integer\"),\n",
    "        ml_monitoring.spec.FieldSchema(\n",
    "            name=\"cnt_level_start_quickplay\", data_type=\"integer\"\n",
    "        ),\n",
    "        ml_monitoring.spec.FieldSchema(\n",
    "            name=\"cnt_level_end_quickplay\", data_type=\"integer\"\n",
    "        ),\n",
    "        ml_monitoring.spec.FieldSchema(\n",
    "            name=\"cnt_level_complete_quickplay\", data_type=\"integer\"\n",
    "        ),\n",
    "        ml_monitoring.spec.FieldSchema(\n",
    "            name=\"cnt_level_reset_quickplay\", data_type=\"integer\"\n",
    "        ),\n",
    "        ml_monitoring.spec.FieldSchema(name=\"cnt_post_score\", data_type=\"integer\"),\n",
    "        ml_monitoring.spec.FieldSchema(\n",
    "            name=\"cnt_spend_virtual_currency\", data_type=\"integer\"\n",
    "        ),\n",
    "        ml_monitoring.spec.FieldSchema(name=\"cnt_ad_reward\", data_type=\"integer\"),\n",
    "        ml_monitoring.spec.FieldSchema(\n",
    "            name=\"cnt_challenge_a_friend\", data_type=\"integer\"\n",
    "        ),\n",
    "        ml_monitoring.spec.FieldSchema(\n",
    "            name=\"cnt_completed_5_levels\", data_type=\"integer\"\n",
    "        ),\n",
    "        ml_monitoring.spec.FieldSchema(name=\"cnt_use_extra_steps\", data_type=\"integer\"),\n",
    "        ml_monitoring.spec.FieldSchema(name=\"month\", data_type=\"categorical\"),\n",
    "        ml_monitoring.spec.FieldSchema(name=\"julianday\", data_type=\"integer\"),\n",
    "        ml_monitoring.spec.FieldSchema(name=\"dayofweek\", data_type=\"integer\"),\n",
    "    ],\n",
    "    ground_truth_fields=[\n",
    "        ml_monitoring.spec.FieldSchema(name=\"churned\", data_type=\"categorical\")\n",
    "    ],\n",
    "    prediction_fields=[\n",
    "        ml_monitoring.spec.FieldSchema(\n",
    "            name=\"predicted_churned\", data_type=\"categorical\"\n",
    "        )\n",
    "    ],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "L8KuANNbiiEV"
   },
   "source": [
    "#### (Optional) Define training dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SvFZRodti3qN"
   },
   "source": [
    "The training dataset can serve as the baseline dataset to calculate monitoring metrics. You can register the training dataset in the model monitor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "CSqE4_roiw-q"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Copying gs://cloud-samples-data/vertex-ai/model-monitoring/churn/churn_training.csv [Content-Type=text/csv]...\n",
      "/ [1 files][717.9 KiB/717.9 KiB]                                                \n",
      "Operation completed over 1 objects/717.9 KiB.                                    \n"
     ]
    }
   ],
   "source": [
    "from vertexai.resources.preview import ml_monitoring\n",
    "\n",
    "# Copy files to your projects gs bucket to avoid permission issues.\n",
    "# Ignore any error(s) for bucket already exists.\n",
    "PUBLIC_TRAINING_DATASET = (\n",
    "    \"gs://cloud-samples-data/vertex-ai/model-monitoring/churn/churn_training.csv\"\n",
    ")\n",
    "TRAINING_URI = f\"{BUCKET_URI}/model-monitoring/churn/churn_training.csv\"\n",
    "\n",
    "! gsutil copy $PUBLIC_TRAINING_DATASET $TRAINING_URI\n",
    "\n",
    "TRAINING_DATASET = ml_monitoring.spec.MonitoringInput(\n",
    "    gcs_uri=TRAINING_URI, data_format=\"csv\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5ZnBZUEMjtDD"
   },
   "source": [
    "#### Create a model monitor resource"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OPBdgqz6jv9_"
   },
   "source": [
    "A model monitor is a top-level resource to manage your metrics and model monitoring jobs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "l_d917Ory595"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating ModelMonitor\n",
      "Create ModelMonitor backing LRO: projects/433578906282/locations/us-central1/modelMonitors/2524496289559740416/operations/5692660739152543744\n",
      "ModelMonitor created. Resource name: projects/433578906282/locations/us-central1/modelMonitors/2524496289559740416\n",
      "To use this ModelMonitor in another session:\n",
      "model_monitor = aiplatform.ModelMonitor('projects/433578906282/locations/us-central1/modelMonitors/2524496289559740416')\n",
      "https://console.cloud.google.com/vertex-ai/model-monitoring/locations/us-central1/model-monitors/2524496289559740416?project=vertex-ai-382806\n",
      "MODEL MONITOR 2524496289559740416 created.\n"
     ]
    }
   ],
   "source": [
    "from vertexai.resources.preview import ml_monitoring\n",
    "\n",
    "my_model_monitor = ml_monitoring.ModelMonitor.create(\n",
    "    project=PROJECT_ID,\n",
    "    location=LOCATION,\n",
    "    display_name=\"churn_model_monitor\",\n",
    "    model_name=model.resource_name,\n",
    "    model_version_id=\"1\",\n",
    "    training_dataset=TRAINING_DATASET,\n",
    "    model_monitoring_schema=MODEL_MONITORING_SCHEMA,\n",
    ")\n",
    "MODEL_MONITOR_ID = my_model_monitor.name\n",
    "print(f\"MODEL MONITOR {MODEL_MONITOR_ID} created.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dkHlwQuVli_2"
   },
   "source": [
    "### Step 4: Run an on-demand model monitoring job"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GpVp3fh3rkov"
   },
   "source": [
    "#### Define the monitoring objective configs\n",
    "\n",
    "For tabular models, Model Monitoring supports the following objectives:\n",
    "\n",
    "*   **Input feature drift detection**\n",
    "\n",
    "    Model Monitoring offers drift analysis for both categorical and numeric feature types, with the following supported metrics:\n",
    "\n",
    "    *    Categorical Feature: `Jensen Shannon Divergence`, `L Infinity`\n",
    "    *    Numeric Feature: `Jensen Shannon Divergence`\n",
    "\n",
    "    You can choose to analyze only the features of interest by specifying them in the `features` fields of the `ml_monitoring.spec.DataDriftSpec` specification. If not specified, all input features in the model schema are analyzed. Additionally, you have the option to set default thresholds for categorical or numeric features, or you can specify thresholds for individual features. If the detected drift surpasses a threshold, an alert is sent through email or another notification channel.\n",
    "\n",
    "*  **Prediction output drift detection**\n",
    "\n",
    "    Similar to input feature drift detection, prediction output drift detection identifies data drift in the prediction outputs.\n",
    "\n",
    "*   **Feature attribution drift detection**\n",
    "\n",
    "    Model Monitoring leverages Vertex Explainable AI to monitor feature attributions. Explainable AI enables you to understand the relative contribution of each feature to a resulting prediction. In essence, it assesses the magnitude of each feature's influence.\n",
    "    You must configure the `Explanation` specification with the feature attribution objectives configuration.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "n49LgtHBfpTE"
   },
   "source": [
    "Input feature drift specification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "CtrsEblNqzLT"
   },
   "outputs": [],
   "source": [
    "from vertexai.resources.preview import ml_monitoring\n",
    "\n",
    "FEATURE_THRESHOLDS = {\n",
    "    \"country\": 0.003,\n",
    "    \"cnt_user_engagement\": 0.004,\n",
    "}\n",
    "\n",
    "FEATURE_DRIFT_SPEC = ml_monitoring.spec.DataDriftSpec(\n",
    "    categorical_metric_type=\"l_infinity\",\n",
    "    numeric_metric_type=\"jensen_shannon_divergence\",\n",
    "    default_categorical_alert_threshold=0.2,\n",
    "    default_numeric_alert_threshold=0.3,\n",
    "    feature_alert_thresholds=FEATURE_THRESHOLDS,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DsuNTFAeftgj"
   },
   "source": [
    "Prediction output drift specification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "tcUz6OFvfoBv"
   },
   "outputs": [],
   "source": [
    "PREDICTION_OUTPUT_DRIFT_SPEC = ml_monitoring.spec.DataDriftSpec(\n",
    "    categorical_metric_type=\"l_infinity\",\n",
    "    numeric_metric_type=\"jensen_shannon_divergence\",\n",
    "    default_categorical_alert_threshold=0.1,\n",
    "    default_numeric_alert_threshold=0.1,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ak54Y713f1aj"
   },
   "source": [
    "Feature attribution specification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "GgIZLRJa5QY4"
   },
   "outputs": [],
   "source": [
    "FEATURE_ATTRIBUTION_SPEC = ml_monitoring.spec.FeatureAttributionSpec(\n",
    "    default_alert_threshold=0.0003,\n",
    "    feature_alert_thresholds={\"cnt_ad_reward\": 0.0001},\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "avV5qcCUr9IT"
   },
   "source": [
    "#### Define the alert notification and metrics output spec."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8N9YlXzOvyOs"
   },
   "source": [
    "Model Monitoring supports the following notification methods:\n",
    "\n",
    "*   Email\n",
    "*   [Notification Channel](https://cloud.google.com/monitoring/support/notification-options)\n",
    "*   [Cloud Logging](https://cloud.google.com/logging/docs)  \n",
    "\n",
    "This notebook uses email as an example."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7jzY1mpdw-CY"
   },
   "source": [
    "Export generated metrics to the Google Cloud Storage location that you specified or, if you don't specify a location, Vertex AI creates a default bucket to use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "yclsJhNrsI-F"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "from vertexai.resources.preview import ml_monitoring\n",
    "\n",
    "EMAIL = \"jvidhi@google.com\"  # @param {type:\"string\"}\n",
    "if os.getenv(\"IS_TESTING\"):\n",
    "    EMAIL = \"jvidhi@google.com\"\n",
    "\n",
    "NOTIFICATION_SPEC = ml_monitoring.spec.NotificationSpec(\n",
    "    user_emails=[EMAIL],\n",
    ")\n",
    "\n",
    "OUTPUT_SPEC = ml_monitoring.spec.OutputSpec(gcs_base_dir=BUCKET_URI)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rQzbfaAFq9mV"
   },
   "source": [
    "#### Run Model Monitoring Jobs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "saA2MVZ-Yhy_"
   },
   "source": [
    "##### **Example 1: Detect feature drift by comparing a batch prediction job with the training dataset.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "B1qxAIA5qvpR"
   },
   "source": [
    "Let's first create a batch prediction job."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "CIujU9oBqwPp"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Copying gs://cloud-samples-data/vertex-ai/model-monitoring/churn/churn_bp_input_1.jsonl [Content-Type=application/octet-stream]...\n",
      "/ [1 files][223.9 KiB/223.9 KiB]                                                \n",
      "Operation completed over 1 objects/223.9 KiB.                                    \n"
     ]
    }
   ],
   "source": [
    "BP_INPUT_URI_1 = f\"{BUCKET_URI}/model-monitoring/churn/churn_bp_input_1.jsonl\"\n",
    "! gsutil copy gs://cloud-samples-data/vertex-ai/model-monitoring/churn/churn_bp_input_1.jsonl $BP_INPUT_URI_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "-ZYgqB8ZmbDo"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating BatchPredictionJob\n",
      "BatchPredictionJob created. Resource name: projects/433578906282/locations/us-central1/batchPredictionJobs/5074002242403565568\n",
      "To use this BatchPredictionJob in another session:\n",
      "bpj = aiplatform.BatchPredictionJob('projects/433578906282/locations/us-central1/batchPredictionJobs/5074002242403565568')\n",
      "View Batch Prediction Job:\n",
      "https://console.cloud.google.com/ai/platform/locations/us-central1/batch-predictions/5074002242403565568?project=433578906282\n",
      "BatchPredictionJob projects/433578906282/locations/us-central1/batchPredictionJobs/5074002242403565568 current state:\n",
      "JobState.JOB_STATE_PENDING\n",
      "BatchPredictionJob projects/433578906282/locations/us-central1/batchPredictionJobs/5074002242403565568 current state:\n",
      "JobState.JOB_STATE_PENDING\n",
      "BatchPredictionJob projects/433578906282/locations/us-central1/batchPredictionJobs/5074002242403565568 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "BatchPredictionJob projects/433578906282/locations/us-central1/batchPredictionJobs/5074002242403565568 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "BatchPredictionJob projects/433578906282/locations/us-central1/batchPredictionJobs/5074002242403565568 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "BatchPredictionJob projects/433578906282/locations/us-central1/batchPredictionJobs/5074002242403565568 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "BatchPredictionJob projects/433578906282/locations/us-central1/batchPredictionJobs/5074002242403565568 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "BatchPredictionJob projects/433578906282/locations/us-central1/batchPredictionJobs/5074002242403565568 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "BatchPredictionJob projects/433578906282/locations/us-central1/batchPredictionJobs/5074002242403565568 current state:\n",
      "JobState.JOB_STATE_SUCCEEDED\n",
      "BatchPredictionJob run completed. Resource name: projects/433578906282/locations/us-central1/batchPredictionJobs/5074002242403565568\n"
     ]
    }
   ],
   "source": [
    "batch_prediction_job_1 = model.batch_predict(\n",
    "    generate_explanation=True,\n",
    "    job_display_name=\"bp_example_1\",\n",
    "    instances_format=\"jsonl\",\n",
    "    machine_type=\"n1-standard-4\",\n",
    "    gcs_source=[BP_INPUT_URI_1],\n",
    "    gcs_destination_prefix=f\"{BUCKET_URI}/bp_output\",\n",
    "    sync=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "S-3ilL5-03k8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ModelMonitoringJob created. Resource name: projects/433578906282/locations/us-central1/modelMonitors/2524496289559740416/modelMonitoringJobs/7813375917580877824\n",
      "To use this ModelMonitoringJob in another session:\n",
      "model_monitoring_job = aiplatform.ModelMonitoringJob('projects/433578906282/locations/us-central1/modelMonitors/2524496289559740416/modelMonitoringJobs/7813375917580877824')\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from vertexai.resources.preview import ml_monitoring\n",
    "\n",
    "TIMESTAMP = pd.Timestamp.utcnow().strftime(\"%Y%m%d%H%M%S\")\n",
    "JOB_DISPLAY_NAME = f\"churn_model_monitoring_job_{TIMESTAMP}\"\n",
    "TARGET_DATASET = ml_monitoring.spec.MonitoringInput(\n",
    "    batch_prediction_job=batch_prediction_job_1.resource_name\n",
    ")\n",
    "model_monitoring_job_1 = my_model_monitor.run(\n",
    "    display_name=JOB_DISPLAY_NAME,\n",
    "    baseline_dataset=TRAINING_DATASET,\n",
    "    target_dataset=TARGET_DATASET,\n",
    "    tabular_objective_spec=ml_monitoring.spec.TabularObjective(\n",
    "        # Input feature drift spec.\n",
    "        feature_drift_spec=FEATURE_DRIFT_SPEC\n",
    "    ),\n",
    "    notification_spec=NOTIFICATION_SPEC,\n",
    "    output_spec=OUTPUT_SPEC,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# !pip install pandas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ihfK5KxgWEXz"
   },
   "source": [
    "##### **Example 2: Detect feature drift and prediction output drift by comparing a batch prediction job with a previous batch prediction job result.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kBhhRjuIa2iv"
   },
   "source": [
    "You can set up multiple objectives within a single model monitoring job. All metrics are computed by using the same baseline and target dataset.\n",
    "\n",
    "Create another batch prediction job and compare it with the batch prediction job created previously."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "IO2sXgSAXu5A"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Copying gs://cloud-samples-data/vertex-ai/model-monitoring/churn/churn_bp_input_2.jsonl [Content-Type=application/octet-stream]...\n",
      "/ [1 files][443.4 KiB/443.4 KiB]                                                \n",
      "Operation completed over 1 objects/443.4 KiB.                                    \n"
     ]
    }
   ],
   "source": [
    "BP_INPUT_URI_2 = f\"{BUCKET_URI}/model-monitoring/churn/churn_bp_input_2.jsonl\"\n",
    "! gsutil copy gs://cloud-samples-data/vertex-ai/model-monitoring/churn/churn_bp_input_2.jsonl $BP_INPUT_URI_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "d6CIK_1rWOT7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating BatchPredictionJob\n",
      "BatchPredictionJob created. Resource name: projects/433578906282/locations/us-central1/batchPredictionJobs/4838561618524635136\n",
      "To use this BatchPredictionJob in another session:\n",
      "bpj = aiplatform.BatchPredictionJob('projects/433578906282/locations/us-central1/batchPredictionJobs/4838561618524635136')\n",
      "View Batch Prediction Job:\n",
      "https://console.cloud.google.com/ai/platform/locations/us-central1/batch-predictions/4838561618524635136?project=433578906282\n",
      "BatchPredictionJob projects/433578906282/locations/us-central1/batchPredictionJobs/4838561618524635136 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "BatchPredictionJob projects/433578906282/locations/us-central1/batchPredictionJobs/4838561618524635136 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "BatchPredictionJob projects/433578906282/locations/us-central1/batchPredictionJobs/4838561618524635136 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "BatchPredictionJob projects/433578906282/locations/us-central1/batchPredictionJobs/4838561618524635136 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "BatchPredictionJob projects/433578906282/locations/us-central1/batchPredictionJobs/4838561618524635136 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "BatchPredictionJob projects/433578906282/locations/us-central1/batchPredictionJobs/4838561618524635136 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "BatchPredictionJob projects/433578906282/locations/us-central1/batchPredictionJobs/4838561618524635136 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "BatchPredictionJob projects/433578906282/locations/us-central1/batchPredictionJobs/4838561618524635136 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "BatchPredictionJob projects/433578906282/locations/us-central1/batchPredictionJobs/4838561618524635136 current state:\n",
      "JobState.JOB_STATE_SUCCEEDED\n",
      "BatchPredictionJob run completed. Resource name: projects/433578906282/locations/us-central1/batchPredictionJobs/4838561618524635136\n"
     ]
    }
   ],
   "source": [
    "batch_prediction_job_2 = model.batch_predict(\n",
    "    job_display_name=\"bp_example_2\",\n",
    "    instances_format=\"jsonl\",\n",
    "    machine_type=\"n1-standard-4\",\n",
    "    gcs_source=[BP_INPUT_URI_2],\n",
    "    gcs_destination_prefix=f\"{BUCKET_URI}/bp_output\",\n",
    "    sync=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "Knp5-4I-WKOa"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ModelMonitoringJob created. Resource name: projects/433578906282/locations/us-central1/modelMonitors/2524496289559740416/modelMonitoringJobs/2048768394546642944\n",
      "To use this ModelMonitoringJob in another session:\n",
      "model_monitoring_job = aiplatform.ModelMonitoringJob('projects/433578906282/locations/us-central1/modelMonitors/2524496289559740416/modelMonitoringJobs/2048768394546642944')\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from vertexai.resources.preview import ml_monitoring\n",
    "\n",
    "TIMESTAMP = pd.Timestamp.utcnow().strftime(\"%Y%m%d%H%M%S\")\n",
    "JOB_DISPLAY_NAME = f\"churn_model_monitoring_job_{TIMESTAMP}\"\n",
    "BASELINE_DATASET = ml_monitoring.spec.MonitoringInput(\n",
    "    batch_prediction_job=batch_prediction_job_1.resource_name\n",
    ")\n",
    "TARGET_DATASET = ml_monitoring.spec.MonitoringInput(\n",
    "    batch_prediction_job=batch_prediction_job_2.resource_name\n",
    ")\n",
    "model_monitoring_job_2 = my_model_monitor.run(\n",
    "    display_name=JOB_DISPLAY_NAME,\n",
    "    baseline_dataset=BASELINE_DATASET,\n",
    "    target_dataset=TARGET_DATASET,\n",
    "    tabular_objective_spec=ml_monitoring.spec.TabularObjective(\n",
    "        # Input feature drift spec.\n",
    "        feature_drift_spec=FEATURE_DRIFT_SPEC,\n",
    "        # Prediction output drift spec.\n",
    "        prediction_output_drift_spec=PREDICTION_OUTPUT_DRIFT_SPEC,\n",
    "    ),\n",
    "    notification_spec=NOTIFICATION_SPEC,\n",
    "    output_spec=OUTPUT_SPEC,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AprOJ3ZchX7z"
   },
   "source": [
    "##### **Example 3: Feature attribution drift detection, compares the batch prediction job with a GCS baseline dataset**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kP7t8p8Ih2a-"
   },
   "source": [
    "For feature attribution monitoring, the dataset is sent to the Vertex AI batch explanation job in the following way:\n",
    "\n",
    "*   Google Cloud Storage -> Sent directly as input to Vertex AI batch explanation job.\n",
    "*   BigQuery table -> Sent directly as input to Vertex AI batch explanation job.\n",
    "*   BigQuery Query -> Not supported.\n",
    "*   Vertex AI batch explanation job -> Input of batch prediction job is used as input for the Vertex AI batch explanation job.\n",
    "*   Vertex AI endpoint logging -> Request logging is used as input for Vertex AI batch explanation job.\n",
    "\n",
    "Check that these datasets meet the requirements for a Vertex AI batch explanation job."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vmT9aF-YxGBS"
   },
   "source": [
    "###### Generate model metadata for Vertex Explainable AI\n",
    "You must specify the explanation specification to use a Vertex AI batch explanation job. Run the following cell to extract metadata from the exported model, which is needed for generating the explanations for a prediction request."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "oLl6q8f4xFK5"
   },
   "outputs": [],
   "source": [
    "from google.cloud.aiplatform_v1beta1.types import (ExplanationMetadata,\n",
    "                                                   ExplanationParameters,\n",
    "                                                   ExplanationSpec)\n",
    "\n",
    "EXPLANATION_SPEC = ExplanationSpec(\n",
    "    parameters=ExplanationParameters(\n",
    "        {\"sampled_shapley_attribution\": {\"path_count\": 2}}\n",
    "    ),\n",
    "    metadata=ExplanationMetadata(\n",
    "        inputs={\n",
    "            \"cnt_ad_reward\": ExplanationMetadata.InputMetadata(\n",
    "                {\n",
    "                    \"input_tensor_name\": \"cnt_ad_reward\",\n",
    "                    \"encoding\": \"IDENTITY\",\n",
    "                    \"modality\": \"numeric\",\n",
    "                }\n",
    "            ),\n",
    "            \"cnt_challenge_a_friend\": ExplanationMetadata.InputMetadata(\n",
    "                {\n",
    "                    \"input_tensor_name\": \"cnt_challenge_a_friend\",\n",
    "                    \"encoding\": \"IDENTITY\",\n",
    "                    \"modality\": \"numeric\",\n",
    "                }\n",
    "            ),\n",
    "            \"cnt_completed_5_levels\": ExplanationMetadata.InputMetadata(\n",
    "                {\n",
    "                    \"input_tensor_name\": \"cnt_completed_5_levels\",\n",
    "                    \"encoding\": \"IDENTITY\",\n",
    "                    \"modality\": \"numeric\",\n",
    "                }\n",
    "            ),\n",
    "            \"cnt_level_complete_quickplay\": ExplanationMetadata.InputMetadata(\n",
    "                {\n",
    "                    \"input_tensor_name\": \"cnt_level_complete_quickplay\",\n",
    "                    \"encoding\": \"IDENTITY\",\n",
    "                    \"modality\": \"numeric\",\n",
    "                }\n",
    "            ),\n",
    "            \"cnt_level_end_quickplay\": ExplanationMetadata.InputMetadata(\n",
    "                {\n",
    "                    \"input_tensor_name\": \"cnt_level_end_quickplay\",\n",
    "                    \"encoding\": \"IDENTITY\",\n",
    "                    \"modality\": \"numeric\",\n",
    "                }\n",
    "            ),\n",
    "            \"cnt_level_reset_quickplay\": ExplanationMetadata.InputMetadata(\n",
    "                {\n",
    "                    \"input_tensor_name\": \"cnt_level_reset_quickplay\",\n",
    "                    \"encoding\": \"IDENTITY\",\n",
    "                    \"modality\": \"numeric\",\n",
    "                }\n",
    "            ),\n",
    "            \"cnt_level_start_quickplay\": ExplanationMetadata.InputMetadata(\n",
    "                {\n",
    "                    \"input_tensor_name\": \"cnt_level_start_quickplay\",\n",
    "                    \"encoding\": \"IDENTITY\",\n",
    "                    \"modality\": \"numeric\",\n",
    "                }\n",
    "            ),\n",
    "            \"cnt_post_score\": ExplanationMetadata.InputMetadata(\n",
    "                {\n",
    "                    \"input_tensor_name\": \"cnt_post_score\",\n",
    "                    \"encoding\": \"IDENTITY\",\n",
    "                    \"modality\": \"numeric\",\n",
    "                }\n",
    "            ),\n",
    "            \"cnt_spend_virtual_currency\": ExplanationMetadata.InputMetadata(\n",
    "                {\n",
    "                    \"input_tensor_name\": \"cnt_spend_virtual_currency\",\n",
    "                    \"encoding\": \"IDENTITY\",\n",
    "                    \"modality\": \"numeric\",\n",
    "                }\n",
    "            ),\n",
    "            \"cnt_use_extra_steps\": ExplanationMetadata.InputMetadata(\n",
    "                {\n",
    "                    \"input_tensor_name\": \"cnt_use_extra_steps\",\n",
    "                    \"encoding\": \"IDENTITY\",\n",
    "                    \"modality\": \"numeric\",\n",
    "                }\n",
    "            ),\n",
    "            \"cnt_user_engagement\": ExplanationMetadata.InputMetadata(\n",
    "                {\n",
    "                    \"input_tensor_name\": \"cnt_user_engagement\",\n",
    "                    \"encoding\": \"IDENTITY\",\n",
    "                    \"modality\": \"numeric\",\n",
    "                }\n",
    "            ),\n",
    "            \"country\": ExplanationMetadata.InputMetadata(\n",
    "                {\n",
    "                    \"input_tensor_name\": \"country\",\n",
    "                    \"encoding\": \"IDENTITY\",\n",
    "                    \"modality\": \"categorical\",\n",
    "                }\n",
    "            ),\n",
    "            \"dayofweek\": ExplanationMetadata.InputMetadata(\n",
    "                {\n",
    "                    \"input_tensor_name\": \"dayofweek\",\n",
    "                    \"encoding\": \"IDENTITY\",\n",
    "                    \"modality\": \"numeric\",\n",
    "                }\n",
    "            ),\n",
    "            \"julianday\": ExplanationMetadata.InputMetadata(\n",
    "                {\n",
    "                    \"input_tensor_name\": \"julianday\",\n",
    "                    \"encoding\": \"IDENTITY\",\n",
    "                    \"modality\": \"numeric\",\n",
    "                }\n",
    "            ),\n",
    "            \"language\": ExplanationMetadata.InputMetadata(\n",
    "                {\n",
    "                    \"input_tensor_name\": \"language\",\n",
    "                    \"encoding\": \"IDENTITY\",\n",
    "                    \"modality\": \"categorical\",\n",
    "                }\n",
    "            ),\n",
    "            \"month\": ExplanationMetadata.InputMetadata(\n",
    "                {\n",
    "                    \"input_tensor_name\": \"month\",\n",
    "                    \"encoding\": \"IDENTITY\",\n",
    "                    \"modality\": \"numeric\",\n",
    "                }\n",
    "            ),\n",
    "            \"operating_system\": ExplanationMetadata.InputMetadata(\n",
    "                {\n",
    "                    \"input_tensor_name\": \"operating_system\",\n",
    "                    \"encoding\": \"IDENTITY\",\n",
    "                    \"modality\": \"categorical\",\n",
    "                }\n",
    "            ),\n",
    "            \"user_pseudo_id\": ExplanationMetadata.InputMetadata(\n",
    "                {\n",
    "                    \"input_tensor_name\": \"user_pseudo_id\",\n",
    "                    \"encoding\": \"IDENTITY\",\n",
    "                    \"modality\": \"categorical\",\n",
    "                }\n",
    "            ),\n",
    "        },\n",
    "        outputs={\n",
    "            \"churned_probs\": ExplanationMetadata.OutputMetadata(\n",
    "                {\"output_tensor_name\": \"churned_probs\"}\n",
    "            )\n",
    "        },\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "id": "hexssbF7wC6A"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Copying gs://cloud-samples-data/vertex-ai/model-monitoring/churn/churn_no_ground_truth.jsonl [Content-Type=application/octet-stream]...\n",
      "/ [1 files][  3.5 MiB/  3.5 MiB]                                                \n",
      "Operation completed over 1 objects/3.5 MiB.                                      \n"
     ]
    }
   ],
   "source": [
    "FEATURE_ATTRIBUTION_BASELINE_DATASET = (\n",
    "    f\"{BUCKET_URI}/model-monitoring/churn/churn_no_ground_truth.jsonl\"\n",
    ")\n",
    "! gsutil cp gs://cloud-samples-data/vertex-ai/model-monitoring/churn/churn_no_ground_truth.jsonl $FEATURE_ATTRIBUTION_BASELINE_DATASET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "id": "2SJU52cymulD"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ModelMonitoringJob created. Resource name: projects/433578906282/locations/us-central1/modelMonitors/2524496289559740416/modelMonitoringJobs/3511312373535211520\n",
      "To use this ModelMonitoringJob in another session:\n",
      "model_monitoring_job = aiplatform.ModelMonitoringJob('projects/433578906282/locations/us-central1/modelMonitors/2524496289559740416/modelMonitoringJobs/3511312373535211520')\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from vertexai.resources.preview import ml_monitoring\n",
    "\n",
    "TIMESTAMP = pd.Timestamp.utcnow().strftime(\"%Y%m%d%H%M%S\")\n",
    "JOB_DISPLAY_NAME = f\"churn_model_monitoring_job_{TIMESTAMP}\"\n",
    "BASELINE_DATASET = ml_monitoring.spec.MonitoringInput(\n",
    "    gcs_uri=FEATURE_ATTRIBUTION_BASELINE_DATASET, data_format=\"jsonl\"\n",
    ")\n",
    "TARGET_DATASET = ml_monitoring.spec.MonitoringInput(\n",
    "    batch_prediction_job=batch_prediction_job_2.resource_name\n",
    ")\n",
    "model_monitoring_job_3 = my_model_monitor.run(\n",
    "    display_name=JOB_DISPLAY_NAME,\n",
    "    baseline_dataset=BASELINE_DATASET,\n",
    "    target_dataset=TARGET_DATASET,\n",
    "    tabular_objective_spec=ml_monitoring.spec.TabularObjective(\n",
    "        # Feature attribution spec.\n",
    "        feature_attribution_spec=FEATURE_ATTRIBUTION_SPEC\n",
    "    ),\n",
    "    # You must have a Explanation spec for feature attribution monitoring.\n",
    "    # You can specify the explanation spec in the Model, Model monitor, or the Model monitoring job.\n",
    "    explanation_spec=EXPLANATION_SPEC,\n",
    "    notification_spec=NOTIFICATION_SPEC,\n",
    "    output_spec=OUTPUT_SPEC,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pzYcMoKTP0rG"
   },
   "source": [
    "##### List Model Monitoring Jobs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "id": "urFYUjaZ7iCP"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[name: \"projects/433578906282/locations/us-central1/modelMonitors/2524496289559740416/modelMonitoringJobs/3511312373535211520\"\n",
       "display_name: \"churn_model_monitoring_job_20240701144740\"\n",
       "model_monitoring_spec {\n",
       "  objective_spec {\n",
       "    tabular_objective {\n",
       "      feature_attribution_spec {\n",
       "        default_alert_condition {\n",
       "          threshold: 0.0003\n",
       "        }\n",
       "        feature_alert_conditions {\n",
       "          key: \"cnt_ad_reward\"\n",
       "          value {\n",
       "            threshold: 0.0001\n",
       "          }\n",
       "        }\n",
       "      }\n",
       "    }\n",
       "    explanation_spec {\n",
       "      parameters {\n",
       "        sampled_shapley_attribution {\n",
       "          path_count: 2\n",
       "        }\n",
       "      }\n",
       "      metadata {\n",
       "        inputs {\n",
       "          key: \"user_pseudo_id\"\n",
       "          value {\n",
       "            input_tensor_name: \"user_pseudo_id\"\n",
       "            encoding: IDENTITY\n",
       "            modality: \"categorical\"\n",
       "          }\n",
       "        }\n",
       "        inputs {\n",
       "          key: \"operating_system\"\n",
       "          value {\n",
       "            input_tensor_name: \"operating_system\"\n",
       "            encoding: IDENTITY\n",
       "            modality: \"categorical\"\n",
       "          }\n",
       "        }\n",
       "        inputs {\n",
       "          key: \"month\"\n",
       "          value {\n",
       "            input_tensor_name: \"month\"\n",
       "            encoding: IDENTITY\n",
       "            modality: \"numeric\"\n",
       "          }\n",
       "        }\n",
       "        inputs {\n",
       "          key: \"language\"\n",
       "          value {\n",
       "            input_tensor_name: \"language\"\n",
       "            encoding: IDENTITY\n",
       "            modality: \"categorical\"\n",
       "          }\n",
       "        }\n",
       "        inputs {\n",
       "          key: \"julianday\"\n",
       "          value {\n",
       "            input_tensor_name: \"julianday\"\n",
       "            encoding: IDENTITY\n",
       "            modality: \"numeric\"\n",
       "          }\n",
       "        }\n",
       "        inputs {\n",
       "          key: \"dayofweek\"\n",
       "          value {\n",
       "            input_tensor_name: \"dayofweek\"\n",
       "            encoding: IDENTITY\n",
       "            modality: \"numeric\"\n",
       "          }\n",
       "        }\n",
       "        inputs {\n",
       "          key: \"country\"\n",
       "          value {\n",
       "            input_tensor_name: \"country\"\n",
       "            encoding: IDENTITY\n",
       "            modality: \"categorical\"\n",
       "          }\n",
       "        }\n",
       "        inputs {\n",
       "          key: \"cnt_user_engagement\"\n",
       "          value {\n",
       "            input_tensor_name: \"cnt_user_engagement\"\n",
       "            encoding: IDENTITY\n",
       "            modality: \"numeric\"\n",
       "          }\n",
       "        }\n",
       "        inputs {\n",
       "          key: \"cnt_use_extra_steps\"\n",
       "          value {\n",
       "            input_tensor_name: \"cnt_use_extra_steps\"\n",
       "            encoding: IDENTITY\n",
       "            modality: \"numeric\"\n",
       "          }\n",
       "        }\n",
       "        inputs {\n",
       "          key: \"cnt_spend_virtual_currency\"\n",
       "          value {\n",
       "            input_tensor_name: \"cnt_spend_virtual_currency\"\n",
       "            encoding: IDENTITY\n",
       "            modality: \"numeric\"\n",
       "          }\n",
       "        }\n",
       "        inputs {\n",
       "          key: \"cnt_post_score\"\n",
       "          value {\n",
       "            input_tensor_name: \"cnt_post_score\"\n",
       "            encoding: IDENTITY\n",
       "            modality: \"numeric\"\n",
       "          }\n",
       "        }\n",
       "        inputs {\n",
       "          key: \"cnt_level_start_quickplay\"\n",
       "          value {\n",
       "            input_tensor_name: \"cnt_level_start_quickplay\"\n",
       "            encoding: IDENTITY\n",
       "            modality: \"numeric\"\n",
       "          }\n",
       "        }\n",
       "        inputs {\n",
       "          key: \"cnt_level_reset_quickplay\"\n",
       "          value {\n",
       "            input_tensor_name: \"cnt_level_reset_quickplay\"\n",
       "            encoding: IDENTITY\n",
       "            modality: \"numeric\"\n",
       "          }\n",
       "        }\n",
       "        inputs {\n",
       "          key: \"cnt_level_end_quickplay\"\n",
       "          value {\n",
       "            input_tensor_name: \"cnt_level_end_quickplay\"\n",
       "            encoding: IDENTITY\n",
       "            modality: \"numeric\"\n",
       "          }\n",
       "        }\n",
       "        inputs {\n",
       "          key: \"cnt_level_complete_quickplay\"\n",
       "          value {\n",
       "            input_tensor_name: \"cnt_level_complete_quickplay\"\n",
       "            encoding: IDENTITY\n",
       "            modality: \"numeric\"\n",
       "          }\n",
       "        }\n",
       "        inputs {\n",
       "          key: \"cnt_completed_5_levels\"\n",
       "          value {\n",
       "            input_tensor_name: \"cnt_completed_5_levels\"\n",
       "            encoding: IDENTITY\n",
       "            modality: \"numeric\"\n",
       "          }\n",
       "        }\n",
       "        inputs {\n",
       "          key: \"cnt_challenge_a_friend\"\n",
       "          value {\n",
       "            input_tensor_name: \"cnt_challenge_a_friend\"\n",
       "            encoding: IDENTITY\n",
       "            modality: \"numeric\"\n",
       "          }\n",
       "        }\n",
       "        inputs {\n",
       "          key: \"cnt_ad_reward\"\n",
       "          value {\n",
       "            input_tensor_name: \"cnt_ad_reward\"\n",
       "            encoding: IDENTITY\n",
       "            modality: \"numeric\"\n",
       "          }\n",
       "        }\n",
       "        outputs {\n",
       "          key: \"churned_probs\"\n",
       "          value {\n",
       "            output_tensor_name: \"churned_probs\"\n",
       "          }\n",
       "        }\n",
       "      }\n",
       "    }\n",
       "    baseline_dataset {\n",
       "      columnized_dataset {\n",
       "        gcs_source {\n",
       "          gcs_uri: \"gs://your-bucket-name-vertex-ai-382806-unique/model-monitoring/churn/churn_no_ground_truth.jsonl\"\n",
       "          format_: JSONL\n",
       "        }\n",
       "      }\n",
       "    }\n",
       "    target_dataset {\n",
       "      batch_prediction_output {\n",
       "        batch_prediction_job: \"projects/433578906282/locations/us-central1/batchPredictionJobs/4838561618524635136\"\n",
       "      }\n",
       "    }\n",
       "  }\n",
       "  notification_spec {\n",
       "    email_config {\n",
       "      user_emails: \"jvidhi@google.com\"\n",
       "    }\n",
       "  }\n",
       "  output_spec {\n",
       "    gcs_base_directory {\n",
       "      output_uri_prefix: \"gs://your-bucket-name-vertex-ai-382806-unique/model_monitoring/2524496289559740416\"\n",
       "    }\n",
       "  }\n",
       "}\n",
       "create_time {\n",
       "  seconds: 1719845260\n",
       "  nanos: 554193000\n",
       "}\n",
       "update_time {\n",
       "  seconds: 1719845260\n",
       "  nanos: 554193000\n",
       "}\n",
       "state: JOB_STATE_RUNNING\n",
       ", name: \"projects/433578906282/locations/us-central1/modelMonitors/2524496289559740416/modelMonitoringJobs/2048768394546642944\"\n",
       "display_name: \"churn_model_monitoring_job_20240701143900\"\n",
       "model_monitoring_spec {\n",
       "  objective_spec {\n",
       "    tabular_objective {\n",
       "      feature_drift_spec {\n",
       "        categorical_metric_type: \"l_infinity\"\n",
       "        numeric_metric_type: \"jensen_shannon_divergence\"\n",
       "        default_categorical_alert_condition {\n",
       "          threshold: 0.2\n",
       "        }\n",
       "        default_numeric_alert_condition {\n",
       "          threshold: 0.3\n",
       "        }\n",
       "        feature_alert_conditions {\n",
       "          key: \"country\"\n",
       "          value {\n",
       "            threshold: 0.003\n",
       "          }\n",
       "        }\n",
       "        feature_alert_conditions {\n",
       "          key: \"cnt_user_engagement\"\n",
       "          value {\n",
       "            threshold: 0.004\n",
       "          }\n",
       "        }\n",
       "      }\n",
       "      prediction_output_drift_spec {\n",
       "        categorical_metric_type: \"l_infinity\"\n",
       "        numeric_metric_type: \"jensen_shannon_divergence\"\n",
       "        default_categorical_alert_condition {\n",
       "          threshold: 0.1\n",
       "        }\n",
       "        default_numeric_alert_condition {\n",
       "          threshold: 0.1\n",
       "        }\n",
       "      }\n",
       "    }\n",
       "    explanation_spec {\n",
       "    }\n",
       "    baseline_dataset {\n",
       "      batch_prediction_output {\n",
       "        batch_prediction_job: \"projects/433578906282/locations/us-central1/batchPredictionJobs/5074002242403565568\"\n",
       "      }\n",
       "    }\n",
       "    target_dataset {\n",
       "      batch_prediction_output {\n",
       "        batch_prediction_job: \"projects/433578906282/locations/us-central1/batchPredictionJobs/4838561618524635136\"\n",
       "      }\n",
       "    }\n",
       "  }\n",
       "  notification_spec {\n",
       "    email_config {\n",
       "      user_emails: \"jvidhi@google.com\"\n",
       "    }\n",
       "  }\n",
       "  output_spec {\n",
       "    gcs_base_directory {\n",
       "      output_uri_prefix: \"gs://your-bucket-name-vertex-ai-382806-unique/model_monitoring/2524496289559740416\"\n",
       "    }\n",
       "  }\n",
       "}\n",
       "create_time {\n",
       "  seconds: 1719844740\n",
       "  nanos: 398384000\n",
       "}\n",
       "update_time {\n",
       "  seconds: 1719844997\n",
       "  nanos: 842729000\n",
       "}\n",
       "state: JOB_STATE_SUCCEEDED\n",
       "job_execution_detail {\n",
       "  objective_status {\n",
       "    key: \"RAW_FEATURE_DRIFT\"\n",
       "    value {\n",
       "    }\n",
       "  }\n",
       "  objective_status {\n",
       "    key: \"PREDICTION_OUTPUT_DRIFT\"\n",
       "    value {\n",
       "    }\n",
       "  }\n",
       "}\n",
       ", name: \"projects/433578906282/locations/us-central1/modelMonitors/2524496289559740416/modelMonitoringJobs/7813375917580877824\"\n",
       "display_name: \"churn_model_monitoring_job_20240701141439\"\n",
       "model_monitoring_spec {\n",
       "  objective_spec {\n",
       "    tabular_objective {\n",
       "      feature_drift_spec {\n",
       "        categorical_metric_type: \"l_infinity\"\n",
       "        numeric_metric_type: \"jensen_shannon_divergence\"\n",
       "        default_categorical_alert_condition {\n",
       "          threshold: 0.2\n",
       "        }\n",
       "        default_numeric_alert_condition {\n",
       "          threshold: 0.3\n",
       "        }\n",
       "        feature_alert_conditions {\n",
       "          key: \"country\"\n",
       "          value {\n",
       "            threshold: 0.003\n",
       "          }\n",
       "        }\n",
       "        feature_alert_conditions {\n",
       "          key: \"cnt_user_engagement\"\n",
       "          value {\n",
       "            threshold: 0.004\n",
       "          }\n",
       "        }\n",
       "      }\n",
       "    }\n",
       "    explanation_spec {\n",
       "    }\n",
       "    baseline_dataset {\n",
       "      columnized_dataset {\n",
       "        gcs_source {\n",
       "          gcs_uri: \"gs://your-bucket-name-vertex-ai-382806-unique/model-monitoring/churn/churn_training.csv\"\n",
       "          format_: CSV\n",
       "        }\n",
       "      }\n",
       "    }\n",
       "    target_dataset {\n",
       "      batch_prediction_output {\n",
       "        batch_prediction_job: \"projects/433578906282/locations/us-central1/batchPredictionJobs/5074002242403565568\"\n",
       "      }\n",
       "    }\n",
       "  }\n",
       "  notification_spec {\n",
       "    email_config {\n",
       "      user_emails: \"jvidhi@google.com\"\n",
       "    }\n",
       "  }\n",
       "  output_spec {\n",
       "    gcs_base_directory {\n",
       "      output_uri_prefix: \"gs://your-bucket-name-vertex-ai-382806-unique/model_monitoring/2524496289559740416\"\n",
       "    }\n",
       "  }\n",
       "}\n",
       "create_time {\n",
       "  seconds: 1719843279\n",
       "  nanos: 373540000\n",
       "}\n",
       "update_time {\n",
       "  seconds: 1719843726\n",
       "  nanos: 484008000\n",
       "}\n",
       "state: JOB_STATE_SUCCEEDED\n",
       "job_execution_detail {\n",
       "  objective_status {\n",
       "    key: \"RAW_FEATURE_DRIFT\"\n",
       "    value {\n",
       "    }\n",
       "  }\n",
       "}\n",
       "]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_model_monitor.list_jobs()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nL10YABxl47v"
   },
   "source": [
    "### Step 5: Wait for the Model Monitoring Job to run and verify the result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LrSU6d0xFfzv"
   },
   "source": [
    "#### Verify results through email"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zw5KodgebDVE"
   },
   "source": [
    "After the model monitoring job begins running, which starts after the batch prediction jobs have finished, you receive an email like the following one:\n",
    "\n",
    "<img src=\"https://services.google.com/fh/files/misc/create_job_email.png\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "37SPm_majLVj"
   },
   "source": [
    "After the monitoring job is complete, if any anomalies are detected, you receive an email similar to the following one:\n",
    "\n",
    "<img src=\"https://services.google.com/fh/files/misc/job_anomalies_email.png\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LPC6ZoerifM6"
   },
   "source": [
    "#### Check monitoring metrics: Google Cloud Console"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4Pw6Z-bVbZaE"
   },
   "source": [
    "To view Model Monitoring metrics in the [Google Cloud Console](https://console.cloud.google.com/vertex-ai/model-monitoring/model-monitors), go to the **Monitoring** tab under **Vertex AI.**\n",
    "\n",
    "<img src=\"https://storage.googleapis.com/cmm-public-data/images/bp_details.gif\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MFZcBnwsdSfa"
   },
   "source": [
    "#### Check monitoring metrics: Cloud Storage bucket\n",
    "\n",
    "Run the following to view Model Monitoring metrics stored in the Cloud Storage bucket.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "id": "NhY20BTjjkVk"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No module named 'tensorflow'\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    my_model_monitor.show_feature_drift_stats(model_monitoring_job_1.name)\n",
    "except Exception as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "id": "qtjLw8rVcC13"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No module named 'tensorflow'\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    my_model_monitor.show_feature_drift_stats(model_monitoring_job_2.name)\n",
    "except Exception as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "id": "-mxYA55nz6tO"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No module named 'tensorflow'\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    my_model_monitor.show_output_drift_stats(model_monitoring_job_2.name)\n",
    "except Exception as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hovSbsmBmEKZ"
   },
   "source": [
    "### Step 6: Clean Up\n",
    "\n",
    "If you no longer need your model monitoring resources, run the following to delete them:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ejHg5PqiJPS2"
   },
   "outputs": [],
   "source": [
    "# Delete the model monitor\n",
    "my_model_monitor.delete(force=True)\n",
    "\n",
    "# Delete the model\n",
    "model.delete()"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "model_monitoring_for_custom_model_batch_prediction_job.ipynb",
   "toc_visible": true
  },
  "environment": {
   "kernel": "conda-base-py",
   "name": "workbench-notebooks.m121",
   "type": "gcloud",
   "uri": "us-docker.pkg.dev/deeplearning-platform-release/gcr.io/workbench-notebooks:m121"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel) (Local)",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
